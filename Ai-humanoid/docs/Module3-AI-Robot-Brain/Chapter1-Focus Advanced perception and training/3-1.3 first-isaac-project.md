---
id: 3-1-3-first-isaac-project
title: 3 1.3 first isaac project
sidebar_label: 3 1.3 first isaac project
sidebar_position: 0
---
# 3.1.4 First Isaac™ Project

Now that your development environment is set up, let's create your first Isaac™ project. This project will demonstrate the integration of Isaac Sim for simulation and Isaac ROS for perception, creating a complete pipeline from virtual sensors to processed data.

## Project Overview

Our first project will create a simple warehouse environment where a robot navigates through aisles, detects objects, and localizes itself. This project will include:

1. A basic warehouse scene in Isaac Sim
2. A simple wheeled robot model
3. Camera and LiDAR sensors on the robot
4. Isaac ROS perception nodes for object detection
5. Basic navigation capabilities

## Creating the Simulation Environment

### Step 1: Launch Isaac Sim

Start Isaac Sim and create a new stage:

```bash
isaac-sim
```

Once Isaac Sim launches:
1. Click "New Stage" to create an empty scene
2. Save the stage as "warehouse_scene.usd" in your project directory
3. Set up the basic environment

### Step 2: Create a Simple Warehouse Environment

In Isaac Sim, we'll create a basic warehouse environment using the Stage and Prim tools:

1. **Create the Floor**:
   - Right-click in the viewport → Create → Cube
   - Scale it to 10x10x0.1 units to create a floor
   - Position at (0, 0, 0)
   - Apply a concrete material from the Material Library

2. **Add Warehouse Shelves**:
   - Create several rectangular prisms for shelves
   - Position them to create aisles (e.g., 2m apart)
   - Add simple box shapes as inventory items

3. **Set Up Lighting**:
   - Add an Environment Light (Window → Create → Environment Light)
   - Adjust intensity to 3000 for realistic warehouse lighting
   - Add a Distant Light for shadows

4. **Create a Sky Dome**:
   - Add a Sky Dome prim for realistic sky environment
   - Configure with realistic sky parameters

### Step 3: Add a Robot Model

For our first project, we'll use a simple differential drive robot:

1. **Import Robot Model**:
   - Download a simple robot model (e.g., TurtleBot3 or create a custom one)
   - Import via Window → Import → Import USD
   - Position the robot at the starting location in the warehouse

2. **Configure Robot Prims**:
   - Set up the robot hierarchy with proper joint articulation
   - Add wheel joints for differential drive
   - Configure physics properties for realistic movement

3. **Add Sensors to Robot**:
   - Add a camera sensor to the robot's front
   - Add a simple LiDAR sensor
   - Configure sensor parameters appropriately

## Setting Up the ROS Bridge

### Step 1: Configure ROS Bridge Extension

1. Enable the ROS Bridge extension:
   - Go to Window → Extensions
   - Search for "ROS" and enable "ROS2 Bridge"

2. Add ROS Bridge to your stage:
   - In the Create menu, search for "ROS Bridge"
   - Add the bridge to your stage
   - Configure the bridge settings (domain ID, etc.)

### Step 2: Configure Robot Sensors for ROS

1. **Camera Configuration**:
   - Select the camera prim on your robot
   - Add the "Isaac ROS Bridge" component
   - Configure to publish to `/camera/image_raw` topic
   - Set appropriate image format and resolution

2. **LiDAR Configuration**:
   - Select the LiDAR prim on your robot
   - Add the "Isaac ROS Bridge" component
   - Configure to publish to `/scan` topic
   - Set appropriate range and resolution parameters

3. **Robot State Publisher**:
   - Add a Robot State Publisher component
   - Configure to publish joint states and TF transforms

## Creating ROS Nodes for Perception

### Step 1: Create a ROS Workspace for Our Project

```bash
# Create project directory
mkdir -p ~/isaac_projects/first_project/src
cd ~/isaac_projects/first_project

# Source ROS and Isaac ROS
source /opt/ros/humble/setup.bash
source ~/isaac_ros_ws/install/setup.bash

# Initialize workspace
colcon build
```

### Step 2: Create a Perception Node

Create a simple perception node that processes camera data:

```python
# ~/isaac_projects/first_project/src/first_isaac_project/first_isaac_project/perception_node.py
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from vision_msgs.msg import Detection2DArray
from cv_bridge import CvBridge
import cv2
import numpy as np

class PerceptionNode(Node):
    def __init__(self):
        super().__init__('perception_node')

        # Create subscriber for camera images
        self.image_subscription = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.image_callback,
            10)

        # Create publisher for detections
        self.detection_publisher = self.create_publisher(
            Detection2DArray,
            '/camera/detections',
            10)

        # Initialize OpenCV bridge
        self.bridge = CvBridge()

        self.get_logger().info('Perception Node Initialized')

    def image_callback(self, msg):
        # Convert ROS Image to OpenCV
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

        # Simple object detection using color thresholding
        hsv = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV)

        # Define range for red objects (example)
        lower_red = np.array([0, 50, 50])
        upper_red = np.array([10, 255, 255])
        mask1 = cv2.inRange(hsv, lower_red, upper_red)

        lower_red = np.array([170, 50, 50])
        upper_red = np.array([180, 255, 255])
        mask2 = cv2.inRange(hsv, lower_red, upper_red)

        mask = mask1 + mask2

        # Find contours
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        # Create detection message
        detection_array = Detection2DArray()
        detection_array.header = msg.header

        for contour in contours:
            if cv2.contourArea(contour) > 100:  # Filter small contours
                x, y, w, h = cv2.boundingRect(contour)

                detection = Detection2D()
                detection.header = msg.header
                detection.bbox.center.x = x + w/2
                detection.bbox.center.y = y + h/2
                detection.bbox.size_x = w
                detection.bbox.size_y = h

                detection_array.detections.append(detection)

        # Publish detections
        self.detection_publisher.publish(detection_array)

        # Optionally publish processed image
        processed_msg = self.bridge.cv2_to_imgmsg(cv_image, encoding='bgr8')
        # In a real implementation, you'd publish to a processed image topic

def main(args=None):
    rclpy.init(args=args)

    perception_node = PerceptionNode()

    try:
        rclpy.spin(perception_node)
    except KeyboardInterrupt:
        pass
    finally:
        perception_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Step 3: Create a Navigation Node

Create a simple navigation node that moves the robot:

```python
# ~/isaac_projects/first_project/src/first_isaac_project/first_isaac_project/navigation_node.py
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import Twist
from sensor_msgs.msg import LaserScan
import math

class NavigationNode(Node):
    def __init__(self):
        super().__init__('navigation_node')

        # Create publisher for velocity commands
        self.cmd_vel_publisher = self.create_publisher(
            Twist,
            '/cmd_vel',
            10)

        # Create subscriber for LiDAR data
        self.scan_subscription = self.create_subscription(
            LaserScan,
            '/scan',
            self.scan_callback,
            10)

        # Timer for control loop
        self.timer = self.create_timer(0.1, self.control_loop)

        # Robot state
        self.obstacle_detected = False
        self.obstacle_distance = float('inf')

        self.get_logger().info('Navigation Node Initialized')

    def scan_callback(self, msg):
        # Check for obstacles in front of robot (±30 degrees)
        min_index = int(len(msg.ranges) * 0.45)  # Front-left
        max_index = int(len(msg.ranges) * 0.55)  # Front-right

        front_ranges = msg.ranges[min_index:max_index]
        valid_ranges = [r for r in front_ranges if r > msg.range_min and r < msg.range_max]

        if valid_ranges:
            self.obstacle_distance = min(valid_ranges)
            self.obstacle_detected = self.obstacle_distance < 1.0  # 1 meter threshold
        else:
            self.obstacle_detected = False
            self.obstacle_distance = float('inf')

    def control_loop(self):
        twist = Twist()

        if self.obstacle_detected:
            # Stop and turn right
            twist.linear.x = 0.0
            twist.angular.z = -0.5  # Turn right
        else:
            # Move forward
            twist.linear.x = 0.5
            twist.angular.z = 0.0

        self.cmd_vel_publisher.publish(twist)

def main(args=None):
    rclpy.init(args=args)

    navigation_node = NavigationNode()

    try:
        rclpy.spin(navigation_node)
    except KeyboardInterrupt:
        pass
    finally:
        navigation_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Step 4: Create Launch File

Create a launch file to start all nodes together:

```python
# ~/isaac_projects/first_project/src/first_isaac_project/first_isaac_project/launch/first_project_launch.py
from launch import LaunchDescription
from launch_ros.actions import Node
from ament_index_python.packages import get_package_share_directory
import os

def generate_launch_description():
    perception_node = Node(
        package='first_isaac_project',
        executable='perception_node',
        name='perception_node',
        output='screen'
    )

    navigation_node = Node(
        package='first_isaac_project',
        executable='navigation_node',
        name='navigation_node',
        output='screen'
    )

    return LaunchDescription([
        perception_node,
        navigation_node
    ])
```

### Step 5: Create Package Configuration

Create the package.xml file:

```xml
<?xml version="1.0"?>
<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>
<package format="3">
  <name>first_isaac_project</name>
  <version>0.0.1</version>
  <description>First Isaac Project for Module 3</description>
  <maintainer email="user@example.com">User</maintainer>
  <license>Apache-2.0</license>

  <depend>rclpy</depend>
  <depend>std_msgs</depend>
  <depend>sensor_msgs</depend>
  <depend>geometry_msgs</depend>
  <depend>vision_msgs</depend>
  <depend>cv_bridge</depend>

  <exec_depend>launch</exec_depend>
  <exec_depend>launch_ros</exec_depend>

  <test_depend>ament_copyright</test_depend>
  <test_depend>ament_flake8</test_depend>
  <test_depend>ament_pep257</test_depend>
  <test_depend>python3-pytest</test_depend>

  <export>
    <build_type>ament_python</build_type>
  </export>
</package>
```

And the setup.py file:

```python
from setuptools import setup
from glob import glob
import os

package_name = 'first_isaac_project'

setup(
    name=package_name,
    version='0.0.1',
    packages=[package_name],
    data_files=[
        ('share/ament_index/resource_index/packages',
            ['resource/' + package_name]),
        ('share/' + package_name, ['package.xml']),
        (os.path.join('share', package_name, 'launch'), glob('launch/*.py')),
    ],
    install_requires=['setuptools'],
    zip_safe=True,
    maintainer='User',
    maintainer_email='user@example.com',
    description='First Isaac Project for Module 3',
    license='Apache-2.0',
    tests_require=['pytest'],
    entry_points={
        'console_scripts': [
            'perception_node = first_isaac_project.perception_node:main',
            'navigation_node = first_isaac_project.navigation_node:main',
        ],
    },
)
```

## Building and Running the Project

### Step 1: Build the Project

```bash
cd ~/isaac_projects/first_project
source /opt/ros/humble/setup.bash
source ~/isaac_ros_ws/install/setup.bash
colcon build --packages-select first_isaac_project
source install/setup.bash
```

### Step 2: Run the Simulation and Nodes

1. **Start Isaac Sim**:
   - Launch Isaac Sim with your warehouse scene
   - Ensure the ROS Bridge is configured and running
   - Make sure the robot is positioned correctly

2. **Run ROS Nodes**:
   ```bash
   ros2 launch first_isaac_project first_project_launch.py
   ```

3. **Monitor Topics**:
   ```bash
   # Check available topics
   ros2 topic list

   # Monitor camera data
   ros2 topic echo /camera/image_raw --field data | head -n 10

   # Monitor LiDAR data
   ros2 topic echo /scan --field ranges | head -n 5
   ```

## Debugging and Troubleshooting

### Common Issues and Solutions

1. **ROS Bridge Connection Issues**:
   - Verify that Isaac Sim ROS Bridge extension is enabled
   - Check that domain IDs match between Isaac Sim and ROS nodes
   - Ensure network settings allow communication

2. **Sensor Data Not Publishing**:
   - Check that sensor prims have ROS bridge components attached
   - Verify topic names match between simulation and ROS nodes
   - Ensure Isaac Sim is actively simulating (play button)

3. **Robot Not Moving**:
   - Check that `/cmd_vel` topic is being published to
   - Verify that the robot in Isaac Sim has differential drive configured
   - Check that ROS bridge is properly configured for actuator control

4. **Perception Node Not Processing Images**:
   - Verify image topic name matches what's published by Isaac Sim
   - Check that OpenCV and cv_bridge are properly installed
   - Ensure image encoding formats match

### Debugging Tools

1. **ROS 2 Tools**:
   ```bash
   # Check node graph
   rqt_graph

   # Monitor topics
   rqt_plot

   # Visualize robot state
   rviz2
   ```

2. **Isaac Sim Debugging**:
   - Use Isaac Sim's built-in debugging tools
   - Check the log window for errors
   - Use the Inspector to verify prim properties

3. **Python Debugging**:
   - Add logging statements to your nodes
   - Use ROS 2 logging: `self.get_logger().info('message')`
   - Check ROS 2 log files in `~/.ros/log/`

## Extending the Project

### Ideas for Enhancement

1. **Advanced Perception**:
   - Replace simple color thresholding with a trained neural network
   - Add 3D object detection using depth data
   - Implement SLAM for mapping and localization

2. **Improved Navigation**:
   - Integrate with ROS 2 Navigation (Nav2) stack
   - Add path planning capabilities
   - Implement obstacle avoidance algorithms

3. **Simulation Enhancement**:
   - Add more realistic warehouse objects
   - Include dynamic obstacles (moving objects)
   - Add lighting variations for more realistic perception

## Exercises

1. **Exercise 1**: Modify the perception node to detect different colored objects by adjusting the HSV color ranges.

2. **Exercise 2**: Add a new sensor to your robot (e.g., IMU) and create a node to process its data.

3. **Exercise 3**: Create a more sophisticated navigation strategy that can follow a predetermined path through the warehouse.

4. **Exercise 4**: Implement a simple mapping system that records the robot's path and detected objects.

## Best Practices

### Simulation Best Practices

1. **Realistic Physics**: Configure physics properties to match real-world values
2. **Sensor Accuracy**: Set sensor parameters to realistic values based on actual hardware
3. **Performance**: Balance simulation quality with real-time performance requirements
4. **Validation**: Regularly validate simulation results against real-world data

### ROS Development Best Practices

1. **Modularity**: Keep nodes focused on single responsibilities
2. **Error Handling**: Implement proper error handling and graceful degradation
3. **Logging**: Use appropriate logging levels for debugging and monitoring
4. **Configuration**: Use ROS parameters for configurable values

## Conclusion

This first Isaac™ project demonstrates the integration of Isaac Sim for realistic simulation and Isaac ROS for hardware-accelerated perception. You've created a complete pipeline that includes:

- A photorealistic warehouse simulation environment
- A robot with camera and LiDAR sensors
- ROS nodes for perception and navigation
- Proper configuration of the ROS bridge

This foundation provides the basis for more complex robotics applications using the NVIDIA Isaac™ platform. In the following chapters, we'll explore more advanced topics including Isaac ROS packages, visual SLAM, and Nav2 integration for humanoid robots.

The project also serves as a template that you can extend with more sophisticated perception algorithms, navigation strategies, and simulation scenarios. The modular design allows you to experiment with different components independently while maintaining the overall system architecture.
"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[701],{6082:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"Module3-AI-Robot-Brain/Chapter2-Nav2 Path planning for bipedal humanoid movement/dnn-inference-with-isaac-ros","title":"dnn inference with isaac ros","description":"Deep Neural Network (DNN) inference represents one of the most computationally intensive tasks in modern robotics applications. Isaac ROS provides GPU-accelerated DNN inference capabilities through its specialized packages, leveraging NVIDIA\'s TensorRT for optimal performance. This chapter explores the architecture, implementation, and optimization of DNN inference pipelines using Isaac ROS, covering everything from model preparation to real-time deployment.","source":"@site/docs/Module3-AI-Robot-Brain/Chapter2-Nav2 Path planning for bipedal humanoid movement/dnn-inference-with-isaac-ros.md","sourceDirName":"Module3-AI-Robot-Brain/Chapter2-Nav2 Path planning for bipedal humanoid movement","slug":"/Module3-AI-Robot-Brain/Chapter2-Nav2 Path planning for bipedal humanoid movement/dnn-inference-with-isaac-ros","permalink":"/docs/Module3-AI-Robot-Brain/Chapter2-Nav2 Path planning for bipedal humanoid movement/dnn-inference-with-isaac-ros","draft":false,"unlisted":false,"editUrl":"https://github.com/areebayaseen15/Ai-Humanoid-textbook/edit/main/docs/Module3-AI-Robot-Brain/Chapter2-Nav2 Path planning for bipedal humanoid movement/dnn-inference-with-isaac-ros.md","tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"id":"dnn-inference-with-isaac-ros","title":"dnn inference with isaac ros","sidebar_label":"dnn inference with isaac ros","sidebar_position":0},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Nav2 Path planning for bipedal humanoid movement","permalink":"/docs/category/chapter-2-nav2-path-planning-for-bipedal-humanoid-movement"},"next":{"title":"introduction to isaac ros","permalink":"/docs/Module3-AI-Robot-Brain/Chapter2-Nav2 Path planning for bipedal humanoid movement/introduction-to-isaac-ros"}}');var o=t(4848),r=t(8453);const a={id:"dnn-inference-with-isaac-ros",title:"dnn inference with isaac ros",sidebar_label:"dnn inference with isaac ros",sidebar_position:0},s="3.4.4 DNN Inference with Isaac ROS",l={},c=[{value:"Understanding Isaac ROS DNN Inference Architecture",id:"understanding-isaac-ros-dnn-inference-architecture",level:2},{value:"Core Components",id:"core-components",level:3},{value:"Processing Pipeline Architecture",id:"processing-pipeline-architecture",level:3},{value:"Memory Management System",id:"memory-management-system",level:3},{value:"Model Preparation and Optimization",id:"model-preparation-and-optimization",level:2},{value:"Converting Models for Isaac ROS",id:"converting-models-for-isaac-ros",level:3},{value:"Model Quantization Techniques",id:"model-quantization-techniques",level:3},{value:"Isaac ROS DNN Inference Implementation",id:"isaac-ros-dnn-inference-implementation",level:2},{value:"Core DNN Inference Node",id:"core-dnn-inference-node",level:3},{value:"Multi-Model Inference Pipelines",id:"multi-model-inference-pipelines",level:2},{value:"Concurrent Model Execution",id:"concurrent-model-execution",level:3},{value:"Performance Optimization Techniques",id:"performance-optimization-techniques",level:2},{value:"GPU Memory Optimization",id:"gpu-memory-optimization",level:3},{value:"Batch Processing Optimization",id:"batch-processing-optimization",level:3},{value:"Real-Time Performance Considerations",id:"real-time-performance-considerations",level:2},{value:"Latency Optimization",id:"latency-optimization",level:3},{value:"Practical Implementation Examples",id:"practical-implementation-examples",level:2},{value:"Object Detection Pipeline",id:"object-detection-pipeline",level:3},{value:"Configuration Files for DNN Inference",id:"configuration-files-for-dnn-inference",level:3},{value:"Launch Files and System Integration",id:"launch-files-and-system-integration",level:2},{value:"Isaac ROS DNN Launch File",id:"isaac-ros-dnn-launch-file",level:3},{value:"Troubleshooting and Optimization",id:"troubleshooting-and-optimization",level:2},{value:"Performance Troubleshooting",id:"performance-troubleshooting",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Model Deployment Best Practices",id:"model-deployment-best-practices",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Conclusion",id:"conclusion",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"344-dnn-inference-with-isaac-ros",children:"3.4.4 DNN Inference with Isaac ROS"})}),"\n",(0,o.jsx)(n.p,{children:"Deep Neural Network (DNN) inference represents one of the most computationally intensive tasks in modern robotics applications. Isaac ROS provides GPU-accelerated DNN inference capabilities through its specialized packages, leveraging NVIDIA's TensorRT for optimal performance. This chapter explores the architecture, implementation, and optimization of DNN inference pipelines using Isaac ROS, covering everything from model preparation to real-time deployment."}),"\n",(0,o.jsx)(n.h2,{id:"understanding-isaac-ros-dnn-inference-architecture",children:"Understanding Isaac ROS DNN Inference Architecture"}),"\n",(0,o.jsx)(n.h3,{id:"core-components",children:"Core Components"}),"\n",(0,o.jsx)(n.p,{children:"The Isaac ROS DNN inference system consists of several interconnected components designed for maximum performance and efficiency:"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"TensorRT Integration"}),": At the core of Isaac ROS DNN inference is NVIDIA's TensorRT, which provides optimized inference engines for deep learning models. TensorRT optimizes models by fusing layers, reducing precision where appropriate, and optimizing memory usage."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Nitros Data Type System"}),": Isaac ROS uses the Nitros system for efficient data transport between nodes, minimizing memory copies and maximizing throughput for GPU-accelerated processing."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"GXF (Graph Execution Framework)"}),": The underlying execution framework provides efficient scheduling and memory management for real-time inference applications."]}),"\n",(0,o.jsx)(n.h3,{id:"processing-pipeline-architecture",children:"Processing Pipeline Architecture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example: Isaac ROS DNN inference pipeline architecture\nclass IsaacROSDNNPipeline:\n    def __init__(self):\n        self.preprocessor = self._initialize_preprocessor()\n        self.tensorrt_engine = self._initialize_tensorrt_engine()\n        self.postprocessor = self._initialize_postprocessor()\n        self.output_formatter = self._initialize_output_formatter()\n\n    def _initialize_preprocessor(self):\n        \"\"\"Initialize input preprocessing pipeline\"\"\"\n        return {\n            'type': 'cuda_preprocessor',\n            'operations': [\n                'resize',\n                'normalize',\n                'format_conversion',\n                'batch_assembly'\n            ],\n            'supported_formats': ['bgr8', 'rgb8', 'mono8'],\n            'max_batch_size': 8\n        }\n\n    def _initialize_tensorrt_engine(self):\n        \"\"\"Initialize TensorRT inference engine\"\"\"\n        import tensorrt as trt\n\n        logger = trt.Logger(trt.Logger.WARNING)\n        builder = trt.Builder(logger)\n\n        # This would be configured with actual model parameters\n        return {\n            'type': 'tensorrt_engine',\n            'precision': 'fp16',  # or 'fp32', 'int8'\n            'max_batch_size': 8,\n            'dynamic_shapes': True,\n            'builder': builder\n        }\n\n    def _initialize_postprocessor(self):\n        \"\"\"Initialize output postprocessing pipeline\"\"\"\n        return {\n            'type': 'cuda_postprocessor',\n            'operations': [\n                'nms',  # Non-maximum suppression\n                'bbox_conversion',\n                'confidence_filtering'\n            ],\n            'supported_formats': ['detection', 'segmentation', 'pose']\n        }\n\n    def process_frame(self, input_data):\n        \"\"\"Process a single frame through the DNN pipeline\"\"\"\n        # Preprocess input\n        preprocessed = self._preprocess(input_data)\n\n        # Run inference\n        raw_output = self._run_inference(preprocessed)\n\n        # Postprocess output\n        final_output = self._postprocess(raw_output)\n\n        return final_output\n\n    def _preprocess(self, input_data):\n        \"\"\"GPU-accelerated input preprocessing\"\"\"\n        # This would use CUDA kernels for:\n        # - Image resizing\n        # - Normalization\n        # - Format conversion\n        # - Batch assembly\n        pass\n\n    def _run_inference(self, preprocessed_data):\n        \"\"\"Execute TensorRT inference\"\"\"\n        # This would interface with the TensorRT engine\n        # for GPU-accelerated inference\n        pass\n\n    def _postprocess(self, raw_output):\n        \"\"\"GPU-accelerated output postprocessing\"\"\"\n        # This would use CUDA kernels for:\n        # - NMS (Non-Maximum Suppression)\n        # - Bounding box conversion\n        # - Confidence filtering\n        pass\n"})}),"\n",(0,o.jsx)(n.h3,{id:"memory-management-system",children:"Memory Management System"}),"\n",(0,o.jsx)(n.p,{children:"Efficient memory management is crucial for high-performance DNN inference:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class IsaacROSMemoryManager:\n    def __init__(self):\n        self.device_memory_pool = None\n        self.host_memory_pool = None\n        self.tensor_cache = {}\n        self.stream_pool = []\n\n    def initialize_memory_system(self, config):\n        """Initialize GPU memory management system"""\n        import pycuda.driver as cuda\n        import pycuda.tools as tools\n\n        # Create memory pools for efficient allocation\n        self.device_memory_pool = tools.DeviceMemoryPool()\n        self.host_memory_pool = tools.PageLockedMemoryPool()\n\n        # Create CUDA streams for asynchronous operations\n        for _ in range(config.get(\'num_streams\', 4)):\n            stream = cuda.Stream()\n            self.stream_pool.append(stream)\n\n    def allocate_tensors(self, input_shape, output_shape, batch_size=1):\n        """Allocate GPU tensors for inference"""\n        import pycuda.gpuarray as gpuarray\n        import numpy as np\n\n        # Calculate memory requirements\n        input_size = np.prod(input_shape) * batch_size * 4  # 4 bytes for float32\n        output_size = np.prod(output_shape) * batch_size * 4\n\n        # Allocate GPU memory\n        input_tensor = gpuarray.empty(input_shape * batch_size, dtype=np.float32)\n        output_tensor = gpuarray.empty(output_shape * batch_size, dtype=np.float32)\n\n        return {\n            \'input\': input_tensor,\n            \'output\': output_tensor,\n            \'input_size\': input_size,\n            \'output_size\': output_size\n        }\n\n    def manage_memory_lifecycle(self, tensors, inference_results):\n        """Manage memory lifecycle for inference operations"""\n        # This would implement:\n        # - Memory reuse strategies\n        # - Automatic cleanup\n        # - Memory leak prevention\n        pass\n'})}),"\n",(0,o.jsx)(n.h2,{id:"model-preparation-and-optimization",children:"Model Preparation and Optimization"}),"\n",(0,o.jsx)(n.h3,{id:"converting-models-for-isaac-ros",children:"Converting Models for Isaac ROS"}),"\n",(0,o.jsx)(n.p,{children:"Isaac ROS supports multiple model formats, with ONNX being the most common:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class IsaacROSModelConverter:\n    def __init__(self):\n        self.supported_formats = ['onnx', 'tensorrt', 'tensorflow', 'pytorch']\n\n    def convert_to_tensorrt(self, model_path, precision='fp16', calibration_data=None):\n        \"\"\"Convert model to TensorRT optimized format\"\"\"\n        import tensorrt as trt\n        import onnx\n\n        # Load ONNX model\n        onnx_model = onnx.load(model_path)\n\n        # Create TensorRT logger\n        logger = trt.Logger(trt.Logger.WARNING)\n        builder = trt.Builder(logger)\n\n        # Create network definition\n        network = builder.create_network(\n            1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n        )\n\n        # Parse ONNX model\n        parser = trt.OnnxParser(network, logger)\n        if not parser.parse_from_file(model_path):\n            for error in range(parser.num_errors):\n                print(parser.get_error(error))\n            return None\n\n        # Configure builder\n        config = builder.create_builder_config()\n        config.max_workspace_size = 2 << 30  # 2GB\n\n        # Set precision\n        if precision == 'fp16':\n            config.set_flag(trt.BuilderFlag.FP16)\n        elif precision == 'int8':\n            config.set_flag(trt.BuilderFlag.INT8)\n            if calibration_data:\n                config.int8_calibrator = self._create_calibrator(calibration_data)\n\n        # Build engine\n        serialized_engine = builder.build_serialized_network(network, config)\n\n        # Save optimized engine\n        engine_path = model_path.replace('.onnx', f'_{precision}.engine')\n        with open(engine_path, 'wb') as f:\n            f.write(serialized_engine)\n\n        return engine_path\n\n    def _create_calibrator(self, calibration_data):\n        \"\"\"Create INT8 calibration data\"\"\"\n        # Implementation would create a TensorRT calibrator\n        # using the provided calibration data\n        pass\n\n    def optimize_model_for_robotics(self, model_path, target_fps, target_latency):\n        \"\"\"Optimize model for robotics-specific requirements\"\"\"\n        # This would implement robotics-specific optimizations:\n        # - Layer fusion\n        # - Precision optimization\n        # - Memory layout optimization\n        # - Real-time constraints\n        pass\n"})}),"\n",(0,o.jsx)(n.h3,{id:"model-quantization-techniques",children:"Model Quantization Techniques"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class IsaacROSQuantization:\n    def __init__(self):\n        self.quantization_methods = {\n            \'post_training_quantization\': self._post_training_quantization,\n            \'quantization_aware_training\': self._quantization_aware_training,\n            \'tensorrt_int8\': self._tensorrt_int8_quantization\n        }\n\n    def _post_training_quantization(self, model, calibration_dataset):\n        """Perform post-training quantization"""\n        import torch\n        import torch.quantization as quant\n\n        # Set model to evaluation mode\n        model.eval()\n\n        # Specify quantization configuration\n        model.qconfig = quant.get_default_qconfig(\'fbgemm\')\n\n        # Prepare model for quantization\n        quant_model = quant.prepare(model, inplace=False)\n\n        # Calibrate the model with sample data\n        with torch.no_grad():\n            for data in calibration_dataset:\n                quant_model(data)\n\n        # Convert to quantized model\n        quant_model = quant.convert(quant_model, inplace=False)\n\n        return quant_model\n\n    def _tensorrt_int8_quantization(self, onnx_model_path, calibration_data):\n        """Perform TensorRT INT8 quantization"""\n        import tensorrt as trt\n\n        logger = trt.Logger(trt.Logger.WARNING)\n        builder = trt.Builder(logger)\n\n        # Create network and parser\n        network = builder.create_network(\n            1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n        )\n        parser = trt.OnnxParser(network, logger)\n\n        if not parser.parse_from_file(onnx_model_path):\n            return None\n\n        # Configure INT8 calibration\n        config = builder.create_builder_config()\n        config.set_flag(trt.BuilderFlag.INT8)\n        config.int8_calibrator = self._create_tensorrt_calibrator(calibration_data)\n\n        # Build INT8 engine\n        serialized_engine = builder.build_serialized_network(network, config)\n\n        return serialized_engine\n\n    def _create_tensorrt_calibrator(self, calibration_data):\n        """Create TensorRT INT8 calibrator"""\n        # This would implement a TensorRT calibrator class\n        # for INT8 quantization\n        pass\n'})}),"\n",(0,o.jsx)(n.h2,{id:"isaac-ros-dnn-inference-implementation",children:"Isaac ROS DNN Inference Implementation"}),"\n",(0,o.jsx)(n.h3,{id:"core-dnn-inference-node",children:"Core DNN Inference Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass IsaacROSDetectionNode(Node):\n    def __init__(self):\n        super().__init__('isaac_ros_detection_node')\n\n        # Initialize components\n        self.bridge = CvBridge()\n\n        # Initialize DNN inference engine\n        self.inference_engine = self._initialize_inference_engine()\n\n        # Create subscribers and publishers\n        self.image_sub = self.create_subscription(\n            Image,\n            'input_image',\n            self.image_callback,\n            10\n        )\n\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            'detections',\n            10\n        )\n\n        # Performance monitoring\n        self.frame_count = 0\n        self.start_time = self.get_clock().now()\n\n        self.get_logger().info('Isaac ROS DNN Detection Node Initialized')\n\n    def _initialize_inference_engine(self):\n        \"\"\"Initialize the DNN inference engine\"\"\"\n        # This would initialize Isaac ROS's GPU-accelerated inference\n        # using TensorRT and CUDA\n        return {\n            'model_path': '/path/to/optimized_model.engine',\n            'input_shape': [1, 3, 640, 640],  # Example for YOLO\n            'confidence_threshold': 0.5,\n            'nms_threshold': 0.4,\n            'max_batch_size': 4,\n            'precision': 'fp16'\n        }\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming images for DNN inference\"\"\"\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Perform inference\n            detections = self._perform_inference(cv_image, msg.header)\n\n            # Publish results\n            self.detection_pub.publish(detections)\n\n            # Update performance metrics\n            self.frame_count += 1\n            if self.frame_count % 30 == 0:  # Log every 30 frames\n                current_time = self.get_clock().now()\n                elapsed = (current_time - self.start_time).nanoseconds / 1e9\n                fps = self.frame_count / elapsed if elapsed > 0 else 0\n                self.get_logger().info(f'Inference FPS: {fps:.2f}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in image processing: {e}')\n\n    def _perform_inference(self, image, header):\n        \"\"\"Perform DNN inference on an image\"\"\"\n        import time\n\n        start_time = time.time()\n\n        # Preprocess image for the model\n        preprocessed = self._preprocess_image(image)\n\n        # Run inference (this would use Isaac ROS GPU acceleration)\n        raw_output = self._run_tensorrt_inference(preprocessed)\n\n        # Postprocess results\n        detections = self._postprocess_results(raw_output, image.shape, header)\n\n        end_time = time.time()\n        inference_time = (end_time - start_time) * 1000  # Convert to ms\n\n        if inference_time > 100:  # Log if inference takes more than 100ms\n            self.get_logger().warn(f'Inference took {inference_time:.2f}ms')\n\n        return detections\n\n    def _preprocess_image(self, image):\n        \"\"\"Preprocess image for DNN inference\"\"\"\n        import cv2\n        import numpy as np\n\n        # Resize image to model input size\n        input_height, input_width = 640, 640  # Example for YOLO\n        resized = cv2.resize(image, (input_width, input_height))\n\n        # Convert BGR to RGB\n        rgb_image = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n        # Normalize to [0, 1] and convert to float32\n        normalized = rgb_image.astype(np.float32) / 255.0\n\n        # Change to NCHW format (batch, channels, height, width)\n        nchw_image = np.transpose(normalized, (2, 0, 1))\n\n        # Add batch dimension\n        batched = np.expand_dims(nchw_image, axis=0)\n\n        return batched\n\n    def _run_tensorrt_inference(self, preprocessed_input):\n        \"\"\"Execute TensorRT inference (conceptual)\"\"\"\n        # In actual Isaac ROS, this would interface with\n        # the optimized TensorRT engine\n        # For demonstration, we'll simulate the process\n\n        # This would actually run on GPU using TensorRT\n        # and return raw detection results\n        pass\n\n    def _postprocess_results(self, raw_output, original_shape, header):\n        \"\"\"Postprocess inference results into ROS messages\"\"\"\n        from vision_msgs.msg import Detection2DArray, Detection2D, ObjectHypothesisWithPose\n\n        detections_msg = Detection2DArray()\n        detections_msg.header = header\n\n        # Parse raw output (format depends on model)\n        # For YOLO, output is typically [batch, num_detections, 6]\n        # where last dimension is [x_center, y_center, width, height, confidence, class_id]\n\n        original_height, original_width = original_shape[:2]\n\n        if raw_output is not None:\n            for detection in raw_output:\n                if detection[4] > self.inference_engine['confidence_threshold']:  # Confidence check\n                    detection_2d = Detection2D()\n\n                    # Convert normalized coordinates to image coordinates\n                    x_center = int(detection[0] * original_width)\n                    y_center = int(detection[1] * original_height)\n                    width = int(detection[2] * original_width)\n                    height = int(detection[3] * original_height)\n\n                    # Set bounding box\n                    detection_2d.bbox.center.x = x_center\n                    detection_2d.bbox.center.y = y_center\n                    detection_2d.bbox.size_x = width\n                    detection_2d.bbox.size_y = height\n\n                    # Set object hypothesis\n                    hypothesis = ObjectHypothesisWithPose()\n                    hypothesis.hypothesis.class_id = str(int(detection[5]))\n                    hypothesis.hypothesis.score = float(detection[4])\n\n                    detection_2d.results.append(hypothesis)\n                    detections_msg.detections.append(detection_2d)\n\n        return detections_msg\n"})}),"\n",(0,o.jsx)(n.h2,{id:"multi-model-inference-pipelines",children:"Multi-Model Inference Pipelines"}),"\n",(0,o.jsx)(n.h3,{id:"concurrent-model-execution",children:"Concurrent Model Execution"}),"\n",(0,o.jsx)(n.p,{children:"Isaac ROS supports running multiple models concurrently for complex perception tasks:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class IsaacROSConcurrentInference:\n    def __init__(self):\n        self.models = {}\n        self.execution_contexts = {}\n        self.memory_pools = {}\n\n    def add_model(self, model_name, model_config):\n        """Add a model to the concurrent inference system"""\n        import tensorrt as trt\n\n        # Load and build TensorRT engine\n        logger = trt.Logger(trt.Logger.WARNING)\n        builder = trt.Builder(logger)\n\n        network = builder.create_network(\n            1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n        )\n\n        # Parse model\n        parser = trt.OnnxParser(network, logger)\n        if parser.parse_from_file(model_config[\'model_path\']):\n            # Configure engine\n            config = builder.create_builder_config()\n            config.max_workspace_size = model_config.get(\'workspace_size\', 2 << 30)\n\n            if model_config.get(\'precision\') == \'fp16\':\n                config.set_flag(trt.BuilderFlag.FP16)\n\n            # Build engine\n            engine = builder.build_engine(network, config)\n            self.models[model_name] = engine\n            self.execution_contexts[model_name] = engine.create_execution_context()\n\n            # Allocate memory pools\n            self.memory_pools[model_name] = self._allocate_model_memory(model_config)\n\n            return True\n        else:\n            return False\n\n    def run_concurrent_inference(self, input_data_map):\n        """Run multiple models concurrently on the same input"""\n        import concurrent.futures\n        import threading\n\n        results = {}\n        futures = {}\n\n        # Use thread pool for concurrent execution\n        with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.models)) as executor:\n            # Submit inference tasks\n            for model_name, input_data in input_data_map.items():\n                if model_name in self.models:\n                    future = executor.submit(\n                        self._run_single_model_inference,\n                        model_name, input_data\n                    )\n                    futures[future] = model_name\n\n            # Collect results\n            for future in concurrent.futures.as_completed(futures):\n                model_name = futures[future]\n                try:\n                    result = future.result()\n                    results[model_name] = result\n                except Exception as e:\n                    print(f"Error in model {model_name}: {e}")\n\n        return results\n\n    def _run_single_model_inference(self, model_name, input_data):\n        """Run inference for a single model"""\n        import pycuda.driver as cuda\n        import pycuda.autoinit\n        import numpy as np\n\n        engine = self.models[model_name]\n        context = self.execution_contexts[model_name]\n        memory_pool = self.memory_pools[model_name]\n\n        # Allocate GPU memory\n        input_size = memory_pool[\'input_size\']\n        output_size = memory_pool[\'output_size\']\n\n        d_input = cuda.mem_alloc(input_size)\n        d_output = cuda.mem_alloc(output_size)\n\n        # Transfer input data to GPU\n        cuda.memcpy_htod(d_input, input_data)\n\n        # Run inference\n        bindings = [int(d_input), int(d_output)]\n        context.execute_v2(bindings)\n\n        # Transfer output data back to CPU\n        output_data = np.empty(output_size // 4, dtype=np.float32)  # 4 bytes per float32\n        cuda.memcpy_dtoh(output_data, d_output)\n\n        # Clean up\n        d_input.free()\n        d_output.free()\n\n        return output_data\n\n    def _allocate_model_memory(self, model_config):\n        """Allocate memory pools for a model"""\n        input_shape = model_config[\'input_shape\']\n        output_shape = model_config[\'output_shape\']\n\n        input_size = np.prod(input_shape) * 4  # 4 bytes per float32\n        output_size = np.prod(output_shape) * 4\n\n        return {\n            \'input_size\': input_size,\n            \'output_size\': output_size,\n            \'input_shape\': input_shape,\n            \'output_shape\': output_shape\n        }\n'})}),"\n",(0,o.jsx)(n.h2,{id:"performance-optimization-techniques",children:"Performance Optimization Techniques"}),"\n",(0,o.jsx)(n.h3,{id:"gpu-memory-optimization",children:"GPU Memory Optimization"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class IsaacROSGPUMemoryOptimizer:\n    def __init__(self):\n        self.memory_allocator = None\n        self.buffer_manager = None\n        self.cache_manager = None\n\n    def optimize_memory_usage(self, model_config):\n        \"\"\"Optimize GPU memory usage for DNN inference\"\"\"\n        import pycuda.driver as cuda\n        import pycuda.tools as tools\n\n        # Calculate memory requirements\n        memory_requirements = self._calculate_memory_requirements(model_config)\n\n        # Implement memory optimization strategies\n        optimization_strategy = self._select_optimization_strategy(\n            memory_requirements\n        )\n\n        # Apply optimization\n        optimized_config = self._apply_memory_optimization(\n            model_config, optimization_strategy\n        )\n\n        return optimized_config\n\n    def _calculate_memory_requirements(self, config):\n        \"\"\"Calculate GPU memory requirements\"\"\"\n        # Calculate memory for:\n        # - Model weights\n        # - Activations\n        # - Input/output buffers\n        # - Workspace memory\n        # - Batch buffers\n\n        requirements = {\n            'model_weights': config.get('model_size_mb', 100) * 1024 * 1024,\n            'activations': self._estimate_activation_memory(config),\n            'io_buffers': self._calculate_io_buffer_size(config),\n            'workspace': config.get('workspace_size', 2 * 1024 * 1024 * 1024),  # 2GB default\n            'batch_buffers': self._calculate_batch_memory(config)\n        }\n\n        return requirements\n\n    def _estimate_activation_memory(self, config):\n        \"\"\"Estimate activation memory requirements\"\"\"\n        # This would analyze the model architecture\n        # to estimate activation memory requirements\n        layers = config.get('layers', 100)  # Estimated number of layers\n        avg_layer_size = config.get('avg_layer_size', 1024 * 1024)  # Estimated average size\n\n        return layers * avg_layer_size\n\n    def _select_optimization_strategy(self, requirements):\n        \"\"\"Select appropriate memory optimization strategy\"\"\"\n        total_required = sum(requirements.values())\n\n        # Get available GPU memory\n        import pycuda.driver as cuda\n        cuda.init()\n        device = cuda.Device(0)\n        attrs = device.get_attributes()\n\n        total_memory = device.total_mem()\n\n        if total_required < total_memory * 0.5:\n            return 'performance'  # Enough memory for performance optimization\n        elif total_required < total_memory * 0.8:\n            return 'balanced'    # Balance between performance and memory\n        else:\n            return 'memory_efficient'  # Prioritize memory efficiency\n\n    def _apply_memory_optimization(self, config, strategy):\n        \"\"\"Apply memory optimization based on strategy\"\"\"\n        optimized_config = config.copy()\n\n        if strategy == 'memory_efficient':\n            # Reduce batch size\n            optimized_config['max_batch_size'] = max(1, config.get('max_batch_size', 4) // 2)\n\n            # Use smaller workspace\n            optimized_config['workspace_size'] = config.get('workspace_size', 2 << 30) // 2\n\n            # Enable layer fusion\n            optimized_config['enable_layer_fusion'] = True\n\n        elif strategy == 'performance':\n            # Use larger workspace for better performance\n            optimized_config['workspace_size'] = min(\n                config.get('workspace_size', 2 << 30) * 2,\n                4 << 30  # Cap at 4GB\n            )\n\n            # Keep larger batch sizes\n            optimized_config['max_batch_size'] = config.get('max_batch_size', 4)\n\n        return optimized_config\n"})}),"\n",(0,o.jsx)(n.h3,{id:"batch-processing-optimization",children:"Batch Processing Optimization"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class IsaacROSBatchProcessor:\n    def __init__(self, max_batch_size=8):\n        self.max_batch_size = max_batch_size\n        self.input_buffer = []\n        self.output_buffer = []\n        self.batch_timer = None\n\n    def add_to_batch(self, input_data, callback=None):\n        """Add input data to batch for processing"""\n        import time\n\n        # Add to input buffer\n        self.input_buffer.append({\n            \'data\': input_data,\n            \'timestamp\': time.time(),\n            \'callback\': callback\n        })\n\n        # Process batch if full\n        if len(self.input_buffer) >= self.max_batch_size:\n            return self.process_batch()\n\n        return None\n\n    def process_batch(self):\n        """Process the current batch of inputs"""\n        if not self.input_buffer:\n            return []\n\n        # Prepare batched input\n        batched_input = self._prepare_batched_input(self.input_buffer)\n\n        # Run inference on batch\n        batched_output = self._run_batch_inference(batched_input)\n\n        # Split output and call callbacks\n        results = self._split_batch_output(batched_output, self.input_buffer)\n\n        # Clear input buffer\n        self.input_buffer = []\n\n        return results\n\n    def _prepare_batched_input(self, input_list):\n        """Prepare input data for batch processing"""\n        import numpy as np\n\n        # Stack inputs along batch dimension\n        # All inputs should have the same shape\n        input_arrays = [item[\'data\'] for item in input_list]\n\n        # Stack along batch dimension (axis 0)\n        batched = np.stack(input_arrays, axis=0)\n\n        return batched\n\n    def _run_batch_inference(self, batched_input):\n        """Run inference on batched input"""\n        # This would interface with Isaac ROS TensorRT engine\n        # for GPU-accelerated batch inference\n        pass\n\n    def _split_batch_output(self, batched_output, input_list):\n        """Split batched output back to individual results"""\n        results = []\n\n        for i, item in enumerate(input_list):\n            # Extract individual result from batch\n            individual_result = batched_output[i]\n\n            # Call callback if provided\n            if item[\'callback\']:\n                item[\'callback\'](individual_result)\n\n            results.append({\n                \'input_id\': i,\n                \'result\': individual_result,\n                \'timestamp\': item[\'timestamp\']\n            })\n\n        return results\n'})}),"\n",(0,o.jsx)(n.h2,{id:"real-time-performance-considerations",children:"Real-Time Performance Considerations"}),"\n",(0,o.jsx)(n.h3,{id:"latency-optimization",children:"Latency Optimization"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class IsaacROSLatencyOptimizer:\n    def __init__(self):\n        self.pipeline_depth = 3  # Number of pipeline stages\n        self.inference_latency = 0\n        self.memory_latency = 0\n        self.communication_latency = 0\n\n    def optimize_for_realtime(self, target_latency_ms=33):  # ~30 FPS\n        """Optimize pipeline for real-time performance"""\n\n        # Analyze current pipeline\n        current_latency = self._measure_current_latency()\n\n        if current_latency > target_latency_ms:\n            # Apply optimization strategies\n            self._apply_latency_optimizations(target_latency_ms)\n\n        return self._get_optimization_report()\n\n    def _measure_current_latency(self):\n        """Measure current pipeline latency"""\n        import time\n\n        # This would measure actual pipeline latency\n        # including preprocessing, inference, and postprocessing\n        start_time = time.time()\n\n        # Simulate pipeline operations\n        # preprocessing_time = self._measure_preprocessing_time()\n        # inference_time = self._measure_inference_time()\n        # postprocessing_time = self._measure_postprocessing_time()\n\n        end_time = time.time()\n        total_time = (end_time - start_time) * 1000  # Convert to ms\n\n        return total_time\n\n    def _apply_latency_optimizations(self, target_latency):\n        """Apply optimizations to meet latency target"""\n\n        # Strategy 1: Reduce input resolution (if acceptable)\n        # self._adjust_input_resolution(target_latency)\n\n        # Strategy 2: Use faster model variant\n        # self._switch_to_lighter_model(target_latency)\n\n        # Strategy 3: Optimize batch size for latency\n        # self._optimize_batch_size_for_latency(target_latency)\n\n        # Strategy 4: Enable TensorRT optimizations\n        self._enable_tensorrt_optimizations()\n\n        # Strategy 5: Optimize memory transfers\n        self._optimize_memory_transfers()\n\n    def _enable_tensorrt_optimizations(self):\n        """Enable TensorRT-specific optimizations"""\n        # Enable layer fusion\n        # Use faster precision (FP16 instead of FP32)\n        # Optimize for latency instead of throughput\n        pass\n\n    def _optimize_memory_transfers(self):\n        """Optimize GPU-CPU memory transfers"""\n        # Use CUDA unified memory\n        # Minimize unnecessary memory copies\n        # Use pinned memory for host transfers\n        pass\n'})}),"\n",(0,o.jsx)(n.h2,{id:"practical-implementation-examples",children:"Practical Implementation Examples"}),"\n",(0,o.jsx)(n.h3,{id:"object-detection-pipeline",children:"Object Detection Pipeline"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Example: Complete object detection pipeline using Isaac ROS\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass IsaacROSObjectDetectionPipeline(Node):\n    def __init__(self):\n        super().__init__(\'isaac_ros_object_detection\')\n\n        self.bridge = CvBridge()\n\n        # Initialize Isaac ROS DNN components\n        self._initialize_dnn_components()\n\n        # Set up ROS interfaces\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/rgb/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            \'/isaac_ros/detections\',\n            10\n        )\n\n        # Performance monitoring\n        self.create_timer(1.0, self._log_performance_metrics)\n        self.frame_count = 0\n        self.start_time = self.get_clock().now().nanoseconds / 1e9\n\n    def _initialize_dnn_components(self):\n        """Initialize Isaac ROS DNN components"""\n        # This would initialize Isaac ROS\'s optimized DNN nodes\n        # using the Nitros type system for efficient data transport\n        self.get_logger().info(\'Initializing Isaac ROS DNN components...\')\n\n        # In actual implementation, this would create:\n        # - Image rectification nodes\n        # - Preprocessing nodes\n        # - TensorRT inference nodes\n        # - Postprocessing nodes\n        # - Result formatting nodes\n\n    def image_callback(self, msg):\n        """Process incoming camera images"""\n        try:\n            # Isaac ROS handles the processing pipeline automatically\n            # when properly configured with Nitros types\n            self.frame_count += 1\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def _log_performance_metrics(self):\n        """Log performance metrics"""\n        current_time = self.get_clock().now().nanoseconds / 1e9\n        elapsed = current_time - self.start_time\n        fps = self.frame_count / elapsed if elapsed > 0 else 0\n\n        self.get_logger().info(f\'Current FPS: {fps:.2f}, Total frames: {self.frame_count}\')\n\n        # Reset for next interval\n        self.frame_count = 0\n        self.start_time = current_time\n'})}),"\n",(0,o.jsx)(n.h3,{id:"configuration-files-for-dnn-inference",children:"Configuration Files for DNN Inference"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'# Example: Isaac ROS DNN inference configuration\nisaac_ros_dnn_inference:\n  object_detection:\n    ros__parameters:\n      # Model configuration\n      engine_file_path: "/models/yolov8n_640x640_fp16.engine"\n      input_tensor_names: ["input"]\n      input_tensor_formats: ["nitros_tensor_list_nchw_rgb_f32"]\n      output_tensor_names: ["output"]\n      output_tensor_formats: ["nitros_tensor_list_nchw_rgb_f32"]\n\n      # Model parameters\n      model_input_width: 640\n      model_input_height: 640\n      confidence_threshold: 0.5\n      max_batch_size: 4\n\n      # TensorRT optimization\n      tensor_rt_precision: "fp16"\n      tensor_rt_engine_cache_path: "/tmp/tensorrt_cache"\n\n      # Performance settings\n      enable_dynamic_batching: true\n      input_queue_size: 1\n      output_queue_size: 1\n\n      # GPU memory management\n      use_device_memory_pool: true\n      device_memory_pool_size: "1GB"\n      host_memory_pool_size: "512MB"\n\n  segmentation:\n    ros__parameters:\n      # Segmentation model configuration\n      engine_file_path: "/models/segmentation_model.engine"\n      input_tensor_names: ["input_tensor"]\n      output_tensor_names: ["output_tensor"]\n      model_input_width: 480\n      model_input_height: 640\n      confidence_threshold: 0.7\n      enable_softmax: true\n'})}),"\n",(0,o.jsx)(n.h2,{id:"launch-files-and-system-integration",children:"Launch Files and System Integration"}),"\n",(0,o.jsx)(n.h3,{id:"isaac-ros-dnn-launch-file",children:"Isaac ROS DNN Launch File"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Example: Isaac ROS DNN inference launch file --\x3e\n<launch>\n  \x3c!-- Declare launch arguments --\x3e\n  <arg name="model_path" default="/models/yolov8n.engine"/>\n  <arg name="input_topic" default="/camera/rgb/image_raw"/>\n  <arg name="output_topic" default="/detections"/>\n  <arg name="confidence_threshold" default="0.5"/>\n\n  \x3c!-- Isaac ROS Image Rectification (if needed) --\x3e\n  <node pkg="isaac_ros_image_proc"\n        exec="rectify_node"\n        name="image_rectifier">\n    <param name="output_queue_size" value="1"/>\n  </node>\n\n  \x3c!-- Isaac ROS DNN Inference Node --\x3e\n  <node pkg="isaac_ros_dnn_inference"\n        exec="trt_engine_tensor_node"\n        name="tensor_rt_engine">\n    <param name="engine_file_path" value="$(var model_path)"/>\n    <param name="input_tensor_names" value="[\'input_tensor\']"/>\n    <param name="output_tensor_names" value="[\'output_tensor\']"/>\n    <param name="input_tensor_formats" value="[\'nitros_tensor_list_nchw_rgb_f32\']"/>\n    <param name="output_tensor_formats" value="[\'nitros_tensor_list_nchw_rgb_f32\']"/>\n    <param name="model_input_width" value="640"/>\n    <param name="model_input_height" value="640"/>\n    <param name="confidence_threshold" value="$(var confidence_threshold)"/>\n  </node>\n\n  \x3c!-- Isaac ROS Detections NITROS Node --\x3e\n  <node pkg="isaac_ros_dnn_inference"\n        exec="detections_nitros_node"\n        name="detections_nitros_node">\n    <param name="tensor_qos" value="SENSOR_DATA"/>\n    <param name="detections_qos" value="SENSOR_DATA"/>\n  </node>\n\n  \x3c!-- Performance monitoring --\x3e\n  <node pkg="isaac_ros_visualization"\n        exec="detection_publisher"\n        name="detection_publisher">\n    <param name="input_detections_topic" value="$(var output_topic)"/>\n  </node>\n</launch>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting-and-optimization",children:"Troubleshooting and Optimization"}),"\n",(0,o.jsx)(n.h3,{id:"performance-troubleshooting",children:"Performance Troubleshooting"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class IsaacROSDiagnosticTool:\n    def __init__(self):\n        self.metrics = {}\n\n    def diagnose_performance_issues(self):\n        \"\"\"Diagnose common performance issues\"\"\"\n        import psutil\n        import GPUtil\n\n        diagnostics = {\n            'gpu_utilization': self._check_gpu_utilization(),\n            'memory_usage': self._check_memory_usage(),\n            'cpu_usage': self._check_cpu_usage(),\n            'bandwidth': self._check_data_bandwidth(),\n            'pipeline_bottlenecks': self._identify_bottlenecks()\n        }\n\n        return diagnostics\n\n    def _check_gpu_utilization(self):\n        \"\"\"Check GPU utilization and memory usage\"\"\"\n        gpus = GPUtil.getGPUs()\n        if gpus:\n            gpu = gpus[0]  # Check first GPU\n            return {\n                'utilization': gpu.load,\n                'memory_used': gpu.memoryUsed,\n                'memory_total': gpu.memoryTotal,\n                'memory_utilization': gpu.memoryUtil\n            }\n        return None\n\n    def _identify_bottlenecks(self):\n        \"\"\"Identify pipeline bottlenecks\"\"\"\n        # This would analyze:\n        # - Node processing times\n        # - Queue depths\n        # - Memory allocation patterns\n        # - GPU utilization patterns\n        pass\n"})}),"\n",(0,o.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,o.jsx)(n.h3,{id:"model-deployment-best-practices",children:"Model Deployment Best Practices"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Model Optimization"}),": Always optimize models using TensorRT before deployment"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Precision Selection"}),": Use FP16 precision when accuracy allows for better performance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Batch Processing"}),": Enable batching when possible to maximize throughput"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Memory Management"}),": Configure appropriate memory pools for your application"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Pipeline Design"}),": Design efficient data flow between components"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Profiling"}),": Regularly profile your inference pipeline to identify bottlenecks"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Latency vs Throughput"}),": Choose optimization strategies based on your requirements"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resource Monitoring"}),": Monitor GPU utilization and memory usage"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Calibration"}),": Properly calibrate INT8 models for optimal accuracy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Testing"}),": Test with real-world data to validate performance"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Exercise 1"}),": Implement a complete object detection pipeline using Isaac ROS DNN inference with a YOLO model and measure real-time performance."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Exercise 2"}),": Optimize a pre-trained model for Isaac ROS using TensorRT and compare performance between different precision settings (FP32, FP16, INT8)."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Exercise 3"}),": Create a multi-model inference pipeline that performs object detection, segmentation, and pose estimation concurrently."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Exercise 4"}),": Implement a batch processing system for Isaac ROS DNN inference and measure throughput improvements."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,o.jsx)(n.p,{children:"Isaac ROS DNN inference provides powerful GPU-accelerated capabilities for deep learning in robotics applications. The combination of TensorRT optimization, efficient memory management, and the Nitros data type system enables real-time inference performance that is essential for robotic perception systems."}),"\n",(0,o.jsx)(n.p,{children:"The key to success with Isaac ROS DNN inference lies in proper model optimization, efficient pipeline design, and appropriate resource configuration. By leveraging TensorRT's optimization capabilities and Isaac ROS's efficient data transport mechanisms, robotics developers can achieve the performance required for real-time robotic applications while maintaining the flexibility of the ROS 2 ecosystem."}),"\n",(0,o.jsx)(n.p,{children:"As we continue through this module, we'll explore how these DNN inference capabilities integrate with other Isaac ROS packages for advanced perception and navigation systems. The foundation established by Isaac ROS DNN inference enables sophisticated robotic applications that can process and understand complex visual information in real-time."})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var i=t(6540);const o={},r=i.createContext(o);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);
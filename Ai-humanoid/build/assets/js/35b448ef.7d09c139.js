"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[2720],{2306:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Module3-AI-Robot-Brain/Chapter4-saac ROS Hardware-accelerated VSLAM (Visual SLAM) and navigation/data-generation-pipelines","title":"data generation pipelines","description":"Data generation pipelines in Isaac Sim provide the infrastructure needed to systematically create large-scale synthetic datasets with consistent quality and comprehensive annotations. These pipelines integrate scene generation, rendering, annotation, and post-processing to create production-ready datasets for robotics applications.","source":"@site/docs/Module3-AI-Robot-Brain/Chapter4-saac ROS Hardware-accelerated VSLAM (Visual SLAM) and navigation/data-generation-pipelines.md","sourceDirName":"Module3-AI-Robot-Brain/Chapter4-saac ROS Hardware-accelerated VSLAM (Visual SLAM) and navigation","slug":"/Module3-AI-Robot-Brain/Chapter4-saac ROS Hardware-accelerated VSLAM (Visual SLAM) and navigation/data-generation-pipelines","permalink":"/docs/Module3-AI-Robot-Brain/Chapter4-saac ROS Hardware-accelerated VSLAM (Visual SLAM) and navigation/data-generation-pipelines","draft":false,"unlisted":false,"editUrl":"https://github.com/areebayaseen15/Ai-Humanoid-textbook/edit/main/docs/Module3-AI-Robot-Brain/Chapter4-saac ROS Hardware-accelerated VSLAM (Visual SLAM) and navigation/data-generation-pipelines.md","tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"id":"data-generation-pipelines","title":"data generation pipelines","sidebar_label":"data generation pipelines","sidebar_position":0},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Isaac ROS Hardware-accelerated VSLAM (Visual SLAM) and navigation","permalink":"/docs/category/chapter-4-isaac-ros-hardware-accelerated-vslam-visual-slam-and-navigation"},"next":{"title":"dataset export and management","permalink":"/docs/Module3-AI-Robot-Brain/Chapter4-saac ROS Hardware-accelerated VSLAM (Visual SLAM) and navigation/dataset-export-and-management"}}');var i=a(4848),r=a(8453);const s={id:"data-generation-pipelines",title:"data generation pipelines",sidebar_label:"data generation pipelines",sidebar_position:0},o="3.3.2 Data Generation Pipelines",l={},c=[{value:"Pipeline Architecture Overview",id:"pipeline-architecture-overview",level:2},{value:"Core Components",id:"core-components",level:3},{value:"Pipeline Workflow",id:"pipeline-workflow",level:3},{value:"Scalability Considerations",id:"scalability-considerations",level:3},{value:"Replicator API for Randomization",id:"replicator-api-for-randomization",level:2},{value:"Basic Replicator Setup",id:"basic-replicator-setup",level:3},{value:"Advanced Randomization Techniques",id:"advanced-randomization-techniques",level:3},{value:"Annotation Generation with Replicator",id:"annotation-generation-with-replicator",level:3},{value:"Automated Scene Variation Systems",id:"automated-scene-variation-systems",level:2},{value:"Procedural Scene Generation",id:"procedural-scene-generation",level:3},{value:"Batch Processing System",id:"batch-processing-system",level:3},{value:"Batch Data Generation",id:"batch-data-generation",level:2},{value:"Parallel Processing Framework",id:"parallel-processing-framework",level:3},{value:"Memory and Storage Optimization",id:"memory-and-storage-optimization",level:3},{value:"Annotation and Labeling Automation",id:"annotation-and-labeling-automation",level:2},{value:"Multi-Modal Annotation System",id:"multi-modal-annotation-system",level:3},{value:"Quality Validation Pipeline",id:"quality-validation-pipeline",level:3},{value:"Pipeline Integration and Workflow",id:"pipeline-integration-and-workflow",level:2},{value:"Complete Pipeline Example",id:"complete-pipeline-example",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Parallel Processing Optimization",id:"parallel-processing-optimization",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Pipeline Design Best Practices",id:"pipeline-design-best-practices",level:3},{value:"Performance Best Practices",id:"performance-best-practices",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"332-data-generation-pipelines",children:"3.3.2 Data Generation Pipelines"})}),"\n",(0,i.jsx)(n.p,{children:"Data generation pipelines in Isaac Sim provide the infrastructure needed to systematically create large-scale synthetic datasets with consistent quality and comprehensive annotations. These pipelines integrate scene generation, rendering, annotation, and post-processing to create production-ready datasets for robotics applications."}),"\n",(0,i.jsx)(n.h2,{id:"pipeline-architecture-overview",children:"Pipeline Architecture Overview"}),"\n",(0,i.jsx)(n.h3,{id:"core-components",children:"Core Components"}),"\n",(0,i.jsx)(n.p,{children:"A comprehensive synthetic data generation pipeline consists of several interconnected components:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Scene Generator"}),": Creates diverse and realistic scenes with appropriate objects, lighting, and environmental conditions\n",(0,i.jsx)(n.strong,{children:"Renderer"}),": Performs photorealistic rendering with accurate physics simulation\n",(0,i.jsx)(n.strong,{children:"Annotation Engine"}),": Generates comprehensive ground truth annotations\n",(0,i.jsx)(n.strong,{children:"Quality Controller"}),": Validates and ensures data quality\n",(0,i.jsx)(n.strong,{children:"Storage Manager"}),": Handles efficient storage and retrieval of large datasets\n",(0,i.jsx)(n.strong,{children:"Task Manager"}),": Coordinates pipeline execution and resource allocation"]}),"\n",(0,i.jsx)(n.h3,{id:"pipeline-workflow",children:"Pipeline Workflow"}),"\n",(0,i.jsx)(n.p,{children:"The typical workflow follows this sequence:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Configuration"}),": Define generation parameters and requirements"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scene Generation"}),": Create diverse scenes with randomization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Rendering"}),": Generate images and sensor data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Annotation"}),": Create ground truth labels and metadata"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validation"}),": Check data quality and consistency"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Storage"}),": Efficiently store processed data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Indexing"}),": Create indices for efficient retrieval"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"scalability-considerations",children:"Scalability Considerations"}),"\n",(0,i.jsx)(n.p,{children:"Modern pipelines must scale to generate millions of images:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example: Scalable pipeline configuration\npipeline_config = {\n    "batch_size": 32,\n    "parallel_rendering": True,\n    "gpu_rendering": True,\n    "distributed_execution": True,\n    "storage_compression": "png_16bit",  # or other compression\n    "output_format": "coco",  # or kitti, nuscenes, etc.\n    "annotation_types": [\n        "2d_bounding_box",\n        "3d_bounding_box",\n        "semantic_segmentation",\n        "instance_segmentation",\n        "depth_map",\n        "optical_flow"\n    ]\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"replicator-api-for-randomization",children:"Replicator API for Randomization"}),"\n",(0,i.jsx)(n.p,{children:"Isaac Sim's Replicator API provides powerful tools for systematic scene randomization and synthetic data generation."}),"\n",(0,i.jsx)(n.h3,{id:"basic-replicator-setup",children:"Basic Replicator Setup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example: Basic Replicator setup for synthetic data generation\nimport omni.replicator.core as rep\n\ndef setup_replicator_pipeline():\n    """Set up the basic Replicator pipeline"""\n\n    # Initialize Replicator\n    rep.orchestrator.setup()\n\n    # Define randomization functions\n    def randomize_lighting():\n        with rep.randomizer:\n            lights = rep.get.light()\n            with lights:\n                rep.modify.pose(\n                    position=rep.distribution.uniform((-5, 5), (-5, 5), (2, 10)),\n                    rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360))\n                )\n                rep.light.intensity(rep.distribution.normal(1000, 200))\n                rep.light.color(rep.distribution.uniform((0.8, 0.8, 0.8), (1.2, 1.2, 1.2)))\n        return lights.node\n\n    def randomize_objects():\n        with rep.randomizer:\n            objects = rep.get.prims(path_pattern="/World/Objects/*")\n            with objects:\n                rep.modify.pose(\n                    position=rep.distribution.uniform((-10, 0, -10), (10, 2, 10)),\n                    rotation=rep.distribution.uniform((0, 0, 0), (360, 360, 360))\n                )\n        return objects.node\n\n    return randomize_lighting, randomize_objects\n'})}),"\n",(0,i.jsx)(n.h3,{id:"advanced-randomization-techniques",children:"Advanced Randomization Techniques"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def advanced_domain_randomization():\n    """Implement advanced domain randomization techniques"""\n\n    # Material randomization\n    def randomize_materials():\n        with rep.randomizer:\n            prims = rep.get.prims(path_pattern="/World/*")\n            with prims:\n                # Randomize albedo\n                albedo_values = [\n                    (0.8, 0.1, 0.1),  # Red\n                    (0.1, 0.8, 0.1),  # Green\n                    (0.1, 0.1, 0.8),  # Blue\n                    (0.8, 0.8, 0.1),  # Yellow\n                    (0.8, 0.1, 0.8),  # Magenta\n                    (0.1, 0.8, 0.8),  # Cyan\n                ]\n                rep.randomizer.material(\n                    albedo=rep.distribution.choice(albedo_values),\n                    roughness=rep.distribution.uniform(0.1, 0.9),\n                    metallic=rep.distribution.uniform(0.0, 0.2)\n                )\n        return prims.node\n\n    # Background randomization\n    def randomize_backgrounds():\n        with rep.randomizer:\n            # Randomize environment textures\n            rep.randomizer.environment(\n                texture=rep.distribution.choice([\n                    "path/to/env1.hdr",\n                    "path/to/env2.hdr",\n                    "path/to/env3.hdr"\n                ])\n            )\n        return rep.get.light(path_pattern="/World/Light")\n\n    # Camera parameter randomization\n    def randomize_camera():\n        with rep.randomizer:\n            camera = rep.get.camera(path_pattern="/World/Camera")\n            with camera:\n                rep.modify.pose(\n                    position=rep.distribution.uniform((-2, 1, -2), (2, 3, 2)),\n                    rotation=rep.distribution.uniform((-10, -180, -10), (10, 180, 10))\n                )\n        return camera.node\n\n    return randomize_materials, randomize_backgrounds, randomize_camera\n'})}),"\n",(0,i.jsx)(n.h3,{id:"annotation-generation-with-replicator",children:"Annotation Generation with Replicator"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def setup_annotation_pipeline():\n    """Set up annotation generation pipeline"""\n\n    # Semantic segmentation\n    with rep.trigger.on_frame(num_frames=100):\n        # Semantic segmentation annotations\n        semantic_annotator = rep.annotators.aaBB2D()\n        semantic_annotator.attach([rep.get.camera()])\n\n        # Instance segmentation\n        instance_annotator = rep.annotators.instance_segmentation()\n        instance_annotator.attach([rep.get.camera()])\n\n        # Bounding boxes\n        bbox_annotator = rep.annotators.bounding_box_2d_stronger()\n        bbox_annotator.attach([rep.get.camera()])\n\n        # Depth maps\n        depth_annotator = rep.annotators.distance_to_camera()\n        depth_annotator.attach([rep.get.camera()])\n\n        # 3D bounding boxes\n        bbox_3d_annotator = rep.annotators.bounding_box_3d()\n        bbox_3d_annotator.attach([rep.get.camera()])\n\ndef generate_annotated_dataset(num_samples=1000):\n    """Generate a complete annotated dataset"""\n\n    # Set up randomization functions\n    lighting_randomizer, object_randomizer = setup_replicator_pipeline()\n    material_randomizer, bg_randomizer, camera_randomizer = advanced_domain_randomization()\n\n    # Set up annotation pipeline\n    setup_annotation_pipeline()\n\n    # Execute pipeline\n    with rep.trigger.on_frame(num_frames=num_samples):\n        # Apply randomizations\n        rep.randomizer.register(lighting_randomizer)\n        rep.randomizer.register(object_randomizer)\n        rep.randomizer.register(material_randomizer)\n        rep.randomizer.register(bg_randomizer)\n        rep.randomizer.register(camera_randomizer)\n\n    # Run the orchestrator\n    rep.orchestrator.run()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"automated-scene-variation-systems",children:"Automated Scene Variation Systems"}),"\n",(0,i.jsx)(n.h3,{id:"procedural-scene-generation",children:"Procedural Scene Generation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example: Procedural scene generation system\nimport random\nimport numpy as np\n\nclass ProceduralSceneGenerator:\n    def __init__(self, scene_template_path):\n        self.scene_template = self.load_template(scene_template_path)\n        self.object_library = self.load_object_library()\n        self.material_library = self.load_material_library()\n\n    def load_template(self, path):\n        """Load scene template with placeholders for variation"""\n        # Load USD template with variation points\n        return path\n\n    def load_object_library(self):\n        """Load library of objects for scene placement"""\n        return {\n            "vehicles": ["car1.usd", "truck1.usd", "bus1.usd"],\n            "pedestrians": ["person1.usd", "person2.usd"],\n            "obstacles": ["box1.usd", "cone1.usd", "barrier1.usd"],\n            "environment": ["tree1.usd", "building1.usd", "sign1.usd"]\n        }\n\n    def load_material_library(self):\n        """Load library of materials for randomization"""\n        return {\n            "asphalt": {"albedo": (0.2, 0.2, 0.2), "roughness": 0.8},\n            "grass": {"albedo": (0.2, 0.6, 0.2), "roughness": 0.6},\n            "concrete": {"albedo": (0.7, 0.7, 0.7), "roughness": 0.5}\n        }\n\n    def generate_scene(self, variation_params=None):\n        """Generate a scene with specified variations"""\n\n        if variation_params is None:\n            variation_params = self.generate_random_params()\n\n        # Create new stage\n        stage = self.create_base_scene()\n\n        # Add objects based on parameters\n        self.add_objects_to_scene(stage, variation_params)\n\n        # Apply environmental conditions\n        self.apply_environmental_conditions(stage, variation_params)\n\n        # Configure lighting\n        self.configure_lighting(stage, variation_params)\n\n        return stage\n\n    def generate_random_params(self):\n        """Generate random parameters for scene variation"""\n        return {\n            "object_count": random.randint(5, 20),\n            "object_types": random.choices(\n                list(self.object_library.keys()),\n                weights=[0.3, 0.2, 0.2, 0.3],\n                k=random.randint(3, 8)\n            ),\n            "weather": random.choice(["clear", "cloudy", "rainy", "foggy"]),\n            "time_of_day": random.choice(["morning", "noon", "afternoon", "evening"]),\n            "location": random.choice(["urban", "suburban", "rural", "indoor"]),\n            "crowd_density": random.uniform(0.1, 0.9),\n            "traffic_density": random.uniform(0.1, 0.8)\n        }\n\n    def add_objects_to_scene(self, stage, params):\n        """Add objects to the scene based on parameters"""\n\n        for obj_type in params["object_types"]:\n            obj_files = self.object_library[obj_type]\n            for _ in range(random.randint(1, 3)):\n                obj_file = random.choice(obj_files)\n                position = self.generate_random_position(stage, obj_type)\n\n                # Add object to scene\n                self.add_object_at_position(stage, obj_file, position)\n\n    def generate_random_position(self, stage, obj_type):\n        """Generate valid random position for object"""\n        # Implement collision checking and valid position generation\n        x = random.uniform(-20, 20)\n        y = 0.1  # Slightly above ground\n        z = random.uniform(-20, 20)\n        return (x, y, z)\n\n    def apply_environmental_conditions(self, stage, params):\n        """Apply environmental conditions based on parameters"""\n\n        if params["weather"] == "rainy":\n            self.add_rain_effects(stage)\n        elif params["weather"] == "foggy":\n            self.add_fog_effects(stage)\n\n        if params["time_of_day"] == "night":\n            self.add_night_lighting(stage)\n\n    def create_base_scene(self):\n        """Create the base scene structure"""\n        # Create a new USD stage with basic environment\n        from pxr import Usd, UsdGeom\n        stage = Usd.Stage.CreateNew(f"temp_scene_{random.randint(1000, 9999)}.usd")\n\n        # Add basic elements\n        world = UsdGeom.Xform.Define(stage, "/World")\n        ground = UsdGeom.Mesh.Define(stage, "/World/Ground")\n\n        return stage\n'})}),"\n",(0,i.jsx)(n.h3,{id:"batch-processing-system",children:"Batch Processing System"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def create_batch_processing_system():\n    """Create a system for batch processing scene variations"""\n\n    def batch_generate_scenes(base_scene, num_variations=1000):\n        """Generate batch of scene variations"""\n\n        for i in range(num_variations):\n            # Generate random parameters\n            params = generate_variation_params()\n\n            # Create variation of base scene\n            variation_scene = create_scene_variation(base_scene, params)\n\n            # Render and annotate\n            rendered_data = render_scene(variation_scene)\n            annotations = generate_annotations(variation_scene, rendered_data)\n\n            # Validate and store\n            if validate_data_quality(rendered_data, annotations):\n                store_data(rendered_data, annotations, f"scene_{i:06d}")\n\n            # Clean up\n            cleanup_scene(variation_scene)\n\n    def generate_variation_params():\n        """Generate parameters for scene variation"""\n        return {\n            "lighting": {\n                "intensity": random.uniform(0.5, 2.0),\n                "color_temp": random.uniform(3000, 8000),\n                "direction": (\n                    random.uniform(-1, 1),\n                    random.uniform(-1, 1),\n                    random.uniform(-1, 1)\n                )\n            },\n            "objects": {\n                "count": random.randint(3, 15),\n                "types": random.choices(["car", "pedestrian", "obstacle"],\n                                      weights=[0.5, 0.3, 0.2], k=5),\n                "positions": [(random.uniform(-10, 10), 0, random.uniform(-10, 10))\n                             for _ in range(random.randint(3, 15))]\n            },\n            "camera": {\n                "position": (\n                    random.uniform(-5, 5),\n                    random.uniform(1, 3),\n                    random.uniform(-5, 5)\n                ),\n                "fov": random.uniform(30, 90)\n            }\n        }\n\n    return batch_generate_scenes\n'})}),"\n",(0,i.jsx)(n.h2,{id:"batch-data-generation",children:"Batch Data Generation"}),"\n",(0,i.jsx)(n.h3,{id:"parallel-processing-framework",children:"Parallel Processing Framework"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import multiprocessing as mp\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\nimport queue\nimport threading\n\nclass BatchDataGenerator:\n    def __init__(self, num_processes=None, batch_size=32):\n        self.num_processes = num_processes or mp.cpu_count()\n        self.batch_size = batch_size\n        self.task_queue = queue.Queue()\n        self.result_queue = queue.Queue()\n\n    def generate_batch(self, config, output_dir, num_samples):\n        """Generate a batch of synthetic data samples"""\n\n        # Split work into chunks\n        chunks = self.split_work(num_samples, self.num_processes)\n\n        with ProcessPoolExecutor(max_workers=self.num_processes) as executor:\n            futures = []\n            for chunk_idx, chunk_size in enumerate(chunks):\n                start_idx = sum(chunks[:chunk_idx])\n                future = executor.submit(\n                    self.worker_process,\n                    config,\n                    output_dir,\n                    start_idx,\n                    chunk_size\n                )\n                futures.append(future)\n\n            # Collect results\n            for future in futures:\n                result = future.result()\n                print(f"Worker completed: {result}")\n\n    def worker_process(self, config, output_dir, start_idx, num_samples):\n        """Worker process for generating data samples"""\n\n        results = []\n        for i in range(num_samples):\n            sample_idx = start_idx + i\n\n            try:\n                # Generate scene\n                scene = self.generate_scene_variation(config)\n\n                # Render scene\n                rendered_data = self.render_scene(scene, config["camera"])\n\n                # Generate annotations\n                annotations = self.generate_annotations(scene, rendered_data)\n\n                # Apply post-processing\n                processed_data = self.post_process(rendered_data, annotations)\n\n                # Save to disk\n                self.save_sample(processed_data, annotations,\n                               f"{output_dir}/sample_{sample_idx:06d}")\n\n                results.append(f"Sample {sample_idx} completed")\n\n            except Exception as e:\n                print(f"Error generating sample {sample_idx}: {e}")\n                results.append(f"Sample {sample_idx} failed: {e}")\n\n        return results\n\n    def split_work(self, total_samples, num_processes):\n        """Split work evenly across processes"""\n        base_size = total_samples // num_processes\n        remainder = total_samples % num_processes\n\n        chunks = [base_size] * num_processes\n        for i in range(remainder):\n            chunks[i] += 1\n\n        return chunks\n\n    def generate_scene_variation(self, config):\n        """Generate a scene variation based on config"""\n        # Implementation would use Isaac Sim APIs\n        # to create a randomized scene\n        pass\n\n    def render_scene(self, scene, camera_config):\n        """Render the scene from specified camera"""\n        # Implementation would use Isaac Sim rendering\n        pass\n\n    def generate_annotations(self, scene, rendered_data):\n        """Generate ground truth annotations"""\n        # Implementation would use Isaac Sim annotation tools\n        pass\n\n    def post_process(self, rendered_data, annotations):\n        """Apply post-processing to generated data"""\n        # Implementation might include compression, format conversion, etc.\n        pass\n\n    def save_sample(self, data, annotations, filename_prefix):\n        """Save a data sample to disk"""\n        # Save image\n        # Save annotations in appropriate format (COCO, KITTI, etc.)\n        # Save metadata\n        pass\n'})}),"\n",(0,i.jsx)(n.h3,{id:"memory-and-storage-optimization",children:"Memory and Storage Optimization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def optimize_batch_generation():\n    """Optimization techniques for batch generation"""\n\n    # Memory management\n    def memory_efficient_rendering():\n        """Render in memory-efficient chunks"""\n        # Use streaming textures\n        # Implement object pooling\n        # Use level-of-detail appropriately\n        pass\n\n    # Storage optimization\n    def efficient_storage_format(data_type):\n        """Choose appropriate storage format based on data type"""\n\n        formats = {\n            "rgb_image": "png_16bit",  # or jpeg with appropriate quality\n            "depth_map": "png_16bit",  # or specialized depth formats\n            "semantic_seg": "png_8bit",  # indexed color format\n            "instance_seg": "png_32bit",  # for instance IDs\n            "annotations": "json",  # COCO format or similar\n            "metadata": "json"  # scene metadata\n        }\n\n        return formats.get(data_type, "png_16bit")\n\n    # Compression strategies\n    def adaptive_compression(scene_complexity):\n        """Apply compression based on scene complexity"""\n\n        if scene_complexity > 0.8:  # High detail scenes\n            return {"format": "png", "quality": 100}\n        elif scene_complexity > 0.5:  # Medium detail\n            return {"format": "png", "quality": 95}\n        else:  # Low detail scenes\n            return {"format": "jpeg", "quality": 90}\n\n    return memory_efficient_rendering, efficient_storage_format, adaptive_compression\n'})}),"\n",(0,i.jsx)(n.h2,{id:"annotation-and-labeling-automation",children:"Annotation and Labeling Automation"}),"\n",(0,i.jsx)(n.h3,{id:"multi-modal-annotation-system",children:"Multi-Modal Annotation System"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class AnnotationGenerator:\n    def __init__(self):\n        self.annotation_types = {\n            "2d_bbox": self.generate_2d_bounding_boxes,\n            "3d_bbox": self.generate_3d_bounding_boxes,\n            "semantic_seg": self.generate_semantic_segmentation,\n            "instance_seg": self.generate_instance_segmentation,\n            "depth": self.generate_depth_maps,\n            "optical_flow": self.generate_optical_flow,\n            "pose": self.generate_poses,\n            "keypoints": self.generate_keypoints\n        }\n\n    def generate_all_annotations(self, scene, rendered_data):\n        """Generate all annotation types for a scene"""\n\n        annotations = {}\n\n        for ann_type, generator_func in self.annotation_types.items():\n            try:\n                annotations[ann_type] = generator_func(scene, rendered_data)\n            except Exception as e:\n                print(f"Error generating {ann_type} annotations: {e}")\n                annotations[ann_type] = None\n\n        return annotations\n\n    def generate_2d_bounding_boxes(self, scene, rendered_data):\n        """Generate 2D bounding box annotations"""\n\n        # Get all prims in the scene\n        prims = self.get_all_prims(scene)\n\n        bboxes = []\n        for prim in prims:\n            if self.is_annotatable(prim):\n                # Project 3D bounding box to 2D\n                bbox_2d = self.project_3d_bbox_to_2d(\n                    prim.GetWorldBound(0, 0)[0],  # Lower bound\n                    prim.GetWorldBound(0, 0)[1],  # Upper bound\n                    rendered_data["camera_matrix"]\n                )\n\n                bboxes.append({\n                    "label": self.get_prim_label(prim),\n                    "bbox": bbox_2d,\n                    "confidence": 1.0  # Perfect confidence in simulation\n                })\n\n        return bboxes\n\n    def generate_3d_bounding_boxes(self, scene, rendered_data):\n        """Generate 3D bounding box annotations"""\n\n        prims = self.get_all_prims(scene)\n\n        bboxes_3d = []\n        for prim in prims:\n            if self.is_annotatable(prim):\n                # Get 3D bounding box in world coordinates\n                lower, upper = prim.GetWorldBound(0, 0)\n\n                center = [(lower[i] + upper[i]) / 2 for i in range(3)]\n                size = [upper[i] - lower[i] for i in range(3)]\n\n                bboxes_3d.append({\n                    "label": self.get_prim_label(prim),\n                    "center": center,\n                    "size": size,\n                    "rotation": self.get_prim_rotation(prim),\n                    "confidence": 1.0\n                })\n\n        return bboxes_3d\n\n    def generate_semantic_segmentation(self, scene, rendered_data):\n        """Generate semantic segmentation masks"""\n\n        # In Isaac Sim, this would use the semantic segmentation renderer\n        # For this example, we\'ll simulate the process\n\n        # Create segmentation mask based on prim materials/labels\n        segmentation_map = np.zeros(\n            (rendered_data["height"], rendered_data["width"]),\n            dtype=np.int32\n        )\n\n        # Render each prim with its semantic label\n        prims = self.get_all_prims(scene)\n        for i, prim in enumerate(prims):\n            if self.is_annotatable(prim):\n                label_id = self.get_semantic_label_id(prim)\n                mask = self.render_prim_mask(prim, rendered_data)\n                segmentation_map[mask] = label_id\n\n        return segmentation_map\n\n    def generate_instance_segmentation(self, scene, rendered_data):\n        """Generate instance segmentation masks"""\n\n        instance_map = np.zeros(\n            (rendered_data["height"], rendered_data["width"]),\n            dtype=np.int32\n        )\n\n        prims = self.get_all_prims(scene)\n        for i, prim in enumerate(prims, 1):  # Start from 1 (0 is background)\n            if self.is_annotatable(prim):\n                mask = self.render_prim_mask(prim, rendered_data)\n                instance_map[mask] = i\n\n        return instance_map\n\n    def generate_depth_maps(self, scene, rendered_data):\n        """Generate depth maps"""\n\n        # In Isaac Sim, this would use the depth camera renderer\n        # For simulation, we\'ll calculate depth from camera parameters\n        depth_map = self.calculate_depth_from_scene(scene, rendered_data)\n\n        return depth_map\n\n    def get_all_prims(self, scene):\n        """Get all prims in the scene"""\n        # This would use Isaac Sim\'s USD API\n        # For example: stage.TraverseAll()\n        pass\n\n    def is_annotatable(self, prim):\n        """Check if a prim should be annotated"""\n        # Check if prim has appropriate schema for annotation\n        # Skip purely decorative elements\n        pass\n\n    def get_prim_label(self, prim):\n        """Get the semantic label for a prim"""\n        # Extract label from prim metadata or naming convention\n        pass\n'})}),"\n",(0,i.jsx)(n.h3,{id:"quality-validation-pipeline",children:"Quality Validation Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class QualityValidator:\n    def __init__(self):\n        self.checks = [\n            self.check_annotation_completeness,\n            self.check_annotation_consistency,\n            self.check_image_quality,\n            self.check_data_format,\n            self.check_statistical_properties\n        ]\n\n    def validate_sample(self, data, annotations, metadata):\n        """Validate a complete data sample"""\n\n        results = {}\n\n        for check_func in self.checks:\n            try:\n                result = check_func(data, annotations, metadata)\n                results[check_func.__name__] = result\n            except Exception as e:\n                results[check_func.__name__] = {\n                    "status": "error",\n                    "message": str(e)\n                }\n\n        # Overall validation result\n        overall_status = all(\n            result.get("status") == "pass"\n            for result in results.values()\n            if isinstance(result, dict)\n        )\n\n        return {\n            "overall_status": "pass" if overall_status else "fail",\n            "details": results\n        }\n\n    def check_annotation_completeness(self, data, annotations, metadata):\n        """Check if annotations are complete"""\n\n        required_annotation_types = ["2d_bbox", "semantic_seg"]\n        present_types = [k for k, v in annotations.items() if v is not None]\n\n        missing_types = set(required_annotation_types) - set(present_types)\n\n        if missing_types:\n            return {\n                "status": "fail",\n                "missing": list(missing_types),\n                "message": f"Missing required annotation types: {missing_types}"\n            }\n\n        return {"status": "pass"}\n\n    def check_annotation_consistency(self, data, annotations, metadata):\n        """Check consistency between different annotation types"""\n\n        if ("2d_bbox" in annotations and\n            "semantic_seg" in annotations and\n            annotations["2d_bbox"] is not None and\n            annotations["semantic_seg"] is not None):\n\n            # Check if bounding boxes are consistent with segmentation\n            bbox_consistent = self.validate_bbox_segmentation_consistency(\n                annotations["2d_bbox"],\n                annotations["semantic_seg"]\n            )\n\n            if not bbox_consistent:\n                return {\n                    "status": "fail",\n                    "message": "Bounding box and segmentation annotations are inconsistent"\n                }\n\n        return {"status": "pass"}\n\n    def check_image_quality(self, data, annotations, metadata):\n        """Check image quality metrics"""\n\n        image = data["rgb"] if isinstance(data, dict) else data\n\n        # Check for common quality issues\n        metrics = {\n            "blur": self.calculate_blur_metric(image),\n            "noise": self.calculate_noise_metric(image),\n            "exposure": self.calculate_exposure_metric(image),\n            "resolution": image.shape if hasattr(image, \'shape\') else None\n        }\n\n        # Define thresholds\n        if metrics["blur"] > 0.5:  # Example threshold\n            return {\n                "status": "fail",\n                "metrics": metrics,\n                "message": f"Image appears blurry (blur score: {metrics[\'blur\']})"\n            }\n\n        return {"status": "pass", "metrics": metrics}\n\n    def calculate_blur_metric(self, image):\n        """Calculate blur metric using Laplacian variance"""\n        import cv2\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n        # Normalize to 0-1 scale (lower is more blurred)\n        return max(0, min(1, 1.0 - laplacian_var / 1000.0))\n\n    def calculate_noise_metric(self, image):\n        """Calculate noise metric"""\n        # Simple noise estimation using image gradients\n        import cv2\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n        # Calculate variance of gradients\n        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n        grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n        noise_estimate = np.std(grad_magnitude)\n        return min(1.0, noise_estimate / 50.0)  # Normalize\n'})}),"\n",(0,i.jsx)(n.h2,{id:"pipeline-integration-and-workflow",children:"Pipeline Integration and Workflow"}),"\n",(0,i.jsx)(n.h3,{id:"complete-pipeline-example",children:"Complete Pipeline Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def create_complete_synthetic_data_pipeline():\n    """Create a complete synthetic data generation pipeline"""\n\n    # Initialize components\n    scene_generator = ProceduralSceneGenerator("templates/urban.usd")\n    annotation_generator = AnnotationGenerator()\n    quality_validator = QualityValidator()\n    batch_generator = BatchDataGenerator()\n\n    def generate_dataset(config):\n        """Generate complete dataset with all components"""\n\n        print(f"Starting dataset generation: {config[\'name\']}")\n        print(f"Target samples: {config[\'num_samples\']}")\n\n        # Generate scenes in batches\n        for batch_idx in range(0, config[\'num_samples\'], config[\'batch_size\']):\n            batch_start = batch_idx\n            batch_end = min(batch_start + config[\'batch_size\'], config[\'num_samples\'])\n            batch_size = batch_end - batch_start\n\n            print(f"Processing batch {batch_start} to {batch_end}")\n\n            # Generate batch of scenes\n            batch_scenes = []\n            for i in range(batch_size):\n                scene = scene_generator.generate_scene()\n                batch_scenes.append(scene)\n\n            # Process each scene in the batch\n            successful_samples = 0\n            for i, scene in enumerate(batch_scenes):\n                try:\n                    # Render scene\n                    rendered_data = render_scene_with_config(scene, config[\'render_config\'])\n\n                    # Generate annotations\n                    annotations = annotation_generator.generate_all_annotations(\n                        scene, rendered_data\n                    )\n\n                    # Validate quality\n                    validation_result = quality_validator.validate_sample(\n                        rendered_data, annotations, config\n                    )\n\n                    if validation_result[\'overall_status\'] == \'pass\':\n                        # Save sample\n                        sample_path = f"{config[\'output_dir\']}/sample_{batch_start + i:06d}"\n                        save_sample(rendered_data, annotations, sample_path)\n                        successful_samples += 1\n                    else:\n                        print(f"Sample {batch_start + i} failed validation: {validation_result}")\n\n                except Exception as e:\n                    print(f"Error processing sample {batch_start + i}: {e}")\n\n            print(f"Batch completed: {successful_samples}/{batch_size} samples saved")\n\n        print("Dataset generation completed!")\n\n    return generate_dataset\n\n# Example usage\npipeline = create_complete_synthetic_data_pipeline()\n\ndataset_config = {\n    "name": "urban_navigation_dataset",\n    "num_samples": 10000,\n    "batch_size": 100,\n    "output_dir": "./datasets/urban_navigation",\n    "render_config": {\n        "resolution": (1920, 1080),\n        "camera_models": ["rgb", "depth", "semantic"],\n        "sensors": ["camera", "lidar"]\n    },\n    "variation_params": {\n        "weather_range": ["clear", "cloudy", "rainy"],\n        "time_range": ["day", "dusk", "night"],\n        "object_density": (5, 20),\n        "lighting_variation": True\n    }\n}\n\n# Generate the dataset\npipeline(dataset_config)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"parallel-processing-optimization",children:"Parallel Processing Optimization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def optimize_pipeline_performance():\n    """Optimization strategies for pipeline performance"""\n\n    # GPU utilization optimization\n    def optimize_gpu_usage():\n        """Optimize GPU usage for rendering"""\n\n        # Use GPU instancing for repeated objects\n        # Implement efficient texture streaming\n        # Use appropriate level of detail\n        # Optimize render passes\n        pass\n\n    # Memory management\n    def memory_efficient_pipeline():\n        """Implement memory-efficient pipeline"""\n\n        # Use memory mapping for large datasets\n        # Implement object pooling\n        # Use streaming for large scenes\n        # Optimize data structures\n        pass\n\n    # I/O optimization\n    def optimize_disk_io():\n        """Optimize disk input/output operations"""\n\n        # Use asynchronous I/O\n        # Implement compression\n        # Use appropriate file formats\n        # Batch I/O operations\n        pass\n\n    return optimize_gpu_usage, memory_efficient_pipeline, optimize_disk_io\n'})}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Exercise 1"}),": Implement a batch data generation pipeline that creates 1000 varied warehouse scenes with proper annotations and validates the quality of generated data."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Exercise 2"}),": Create a domain randomization system that systematically varies lighting, materials, and object positions for a specific robotics task."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Exercise 3"}),": Develop a quality validation pipeline that checks synthetic data for completeness, consistency, and quality metrics."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Exercise 4"}),": Design and implement a multi-modal annotation system that generates 2D/3D bounding boxes, segmentation masks, and depth maps simultaneously."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"pipeline-design-best-practices",children:"Pipeline Design Best Practices"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Modular Design"}),": Keep pipeline components modular and reusable"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Configuration-Driven"}),": Use configuration files for pipeline parameters"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error Handling"}),": Implement comprehensive error handling and recovery"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Logging and Monitoring"}),": Maintain detailed logs of pipeline execution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Quality Assurance"}),": Include validation at every stage of the pipeline"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"performance-best-practices",children:"Performance Best Practices"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Parallel Processing"}),": Maximize parallel processing opportunities"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Resource Management"}),": Efficiently manage GPU, CPU, and memory resources"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Storage Optimization"}),": Use appropriate compression and storage formats"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Caching"}),": Implement intelligent caching for repeated operations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitoring"}),": Monitor performance metrics and optimize accordingly"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"Data generation pipelines in Isaac Sim provide the infrastructure needed to systematically create large-scale, high-quality synthetic datasets for robotics applications. The combination of procedural scene generation, systematic randomization through Replicator, comprehensive annotation systems, and quality validation creates a robust pipeline for synthetic data generation."}),"\n",(0,i.jsx)(n.p,{children:"The modular architecture allows for customization to specific robotics tasks while maintaining efficiency and scalability. As we continue through this module, we'll explore advanced techniques for domain randomization and the practical tools available in Isaac Sim for creating production-ready synthetic datasets that enable effective sim-to-real transfer of robotic perception systems."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>o});var t=a(6540);const i={},r=t.createContext(i);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);
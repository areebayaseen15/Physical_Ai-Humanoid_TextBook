"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[3900],{5687:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>d,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"Module4-VisionLanguageAction/VisionLAnguageAction","title":"Module 4 \u2014 Implementation Scaffold (Vision \xb7 Language \xb7 Action)","description":"This document contains a ready-to-copy project scaffold and Docusaurus-compatible docs for implementing Module 4 (Vision\u2011Language\u2011Action) from the plan you provided. Use this as the starting repo for the module, capstone, and hackathon-ready deliverables.","source":"@site/docs/Module4-VisionLanguageAction/VisionLAnguageAction.md","sourceDirName":"Module4-VisionLanguageAction","slug":"/Module4-VisionLanguageAction/VisionLAnguageAction","permalink":"/docs/Module4-VisionLanguageAction/VisionLAnguageAction","draft":false,"unlisted":false,"editUrl":"https://github.com/areebayaseen15/Ai-Humanoid-textbook/edit/main/docs/Module4-VisionLanguageAction/VisionLAnguageAction.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"importance of synthetic data","permalink":"/docs/Module3-AI-Robot-Brain/Chapter4-saac ROS Hardware-accelerated VSLAM (Visual SLAM) and navigation/importance-of-synthetic-data"}}');var o=r(4848),t=r(8453);const s={},d="Module 4 \u2014 Implementation Scaffold (Vision \xb7 Language \xb7 Action)",a={},l=[{value:"What\u2019s included",id:"whats-included",level:2},{value:"Project file tree",id:"project-file-tree",level:2},{value:"Key files (content previews)",id:"key-files-content-previews",level:2},{value:"<code>README.md</code> (root)",id:"readmemd-root",level:3},{value:"<code>requirements.txt</code>",id:"requirementstxt",level:3},{value:"<code>Dockerfile</code>",id:"dockerfile",level:3},{value:"<code>docs/</code> pages (Docusaurus-ready markdown)",id:"docs-pages-docusaurus-ready-markdown",level:2},{value:"Implementation stubs (excerpts)",id:"implementation-stubs-excerpts",level:2},{value:"<code>module4/voice_to_action/whisper_integration.py</code>",id:"module4voice_to_actionwhisper_integrationpy",level:3},{value:"<code>module4/voice_to_action/ros2_bridge.py</code>",id:"module4voice_to_actionros2_bridgepy",level:3},{value:"<code>module4/planner/llm_planner.py</code>",id:"module4plannerllm_plannerpy",level:3},{value:"<code>module4/vision/detector.py</code>",id:"module4visiondetectorpy",level:3},{value:"<code>module4/demo/app.py</code> (Gradio demo tying everything together)",id:"module4demoapppy-gradio-demo-tying-everything-together",level:3},{value:"<code>docs/module4-specs.md</code> (polished Docusaurus page)",id:"docsmodule4-specsmd-polished-docusaurus-page",level:2},{value:"CHECKLIST.md (submission)",id:"checklistmd-submission",level:2},{value:"How I generated these files",id:"how-i-generated-these-files",level:2},{value:"Next steps (pick one)",id:"next-steps-pick-one",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4--implementation-scaffold-vision--language--action",children:"Module 4 \u2014 Implementation Scaffold (Vision \xb7 Language \xb7 Action)"})}),"\n",(0,o.jsxs)(n.p,{children:["This document contains a ready-to-copy ",(0,o.jsx)(n.strong,{children:"project scaffold"})," and Docusaurus-compatible docs for implementing Module 4 (Vision\u2011Language\u2011Action) from the plan you provided. Use this as the starting repo for the module, capstone, and hackathon-ready deliverables."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"whats-included",children:"What\u2019s included"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"README.md"})," \u2014 high-level runbook and project goals"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"docs/"})," \u2014 Docusaurus-ready markdown pages (chapter pages + specs)"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"module4/"})," \u2014 implementation scaffold with code stubs and helper scripts"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"module4/voice_to_action/"})," \u2014 Whisper integration and ROS2 bridges"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"module4/planner/"})," \u2014 LLM planning stubs and examples"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"module4/vision/"})," \u2014 CV model wrapper and grounding helpers"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"module4/demo/"})," \u2014 simple Gradio demo that ties perception \u2192 language \u2192 action"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"requirements.txt"})," and ",(0,o.jsx)(n.code,{children:"Dockerfile"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"sample_inputs/"})," \u2014 example audio and images for testing"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"CHECKLIST.md"})," \u2014 submission & judging checklist"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"project-file-tree",children:"Project file tree"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"module4-vla/\r\n\u251c\u2500 README.md\r\n\u251c\u2500 requirements.txt\r\n\u251c\u2500 Dockerfile\r\n\u251c\u2500 docs/\r\n\u2502  \u251c\u2500 module4-specs.md\r\n\u2502  \u251c\u2500 chapter-4-1-voice-to-action.md\r\n\u2502  \u251c\u2500 chapter-4-2-cognitive-planning.md\r\n\u2502  \u2514\u2500 chapter-4-3-capstone.md\r\n\u251c\u2500 module4/\r\n\u2502  \u251c\u2500 voice_to_action/\r\n\u2502  \u2502  \u251c\u2500 README.md\r\n\u2502  \u2502  \u251c\u2500 whisper_integration.py\r\n\u2502  \u2502  \u251c\u2500 ros2_bridge.py\r\n\u2502  \u2502  \u2514\u2500 audio_utils.py\r\n\u2502  \u251c\u2500 planner/\r\n\u2502  \u2502  \u251c\u2500 README.md\r\n\u2502  \u2502  \u251c\u2500 llm_planner.py\r\n\u2502  \u2502  \u2514\u2500 action_schema.json\r\n\u2502  \u251c\u2500 vision/\r\n\u2502  \u2502  \u251c\u2500 README.md\r\n\u2502  \u2502  \u251c\u2500 detector.py\r\n\u2502  \u2502  \u2514\u2500 grounding.py\r\n\u2502  \u2514\u2500 demo/\r\n\u2502     \u251c\u2500 app.py  # Gradio demo\r\n\u2502     \u2514\u2500 static/\r\n\u251c\u2500 sample_inputs/\r\n\u2502  \u251c\u2500 sample1.wav\r\n\u2502  \u2514\u2500 sample1.jpg\r\n\u2514\u2500 CHECKLIST.md\n"})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"key-files-content-previews",children:"Key files (content previews)"}),"\n",(0,o.jsxs)(n.h3,{id:"readmemd-root",children:[(0,o.jsx)(n.code,{children:"README.md"})," (root)"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"# Module 4 \u2014 Vision \xb7 Language \xb7 Action (VLA)\r\n\r\nThis repository contains a scaffold to implement Module 4 from the course book.\r\n\r\nQuick start (CPU-friendly demo):\r\n\r\n1. Create virtual environment: `python -m venv .venv && source .venv/bin/activate`\r\n2. Install dependencies: `pip install -r requirements.txt`\r\n3. Run demo: `python module4/demo/app.py`\r\n\r\nFor ROS 2 integration, see `module4/voice_to_action/ros2_bridge.py` and the README in that folder.\r\n\r\nSee `docs/module4-specs.md` for the full module spec.\n"})}),"\n",(0,o.jsx)(n.h3,{id:"requirementstxt",children:(0,o.jsx)(n.code,{children:"requirements.txt"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"# minimal demo stack\r\ngradio==3.40.0\r\ntorch>=1.12\r\ntransformers>=4.20.0\r\ntorchaudio\r\nopenai\r\nwhisper @ git+https://github.com/openai/whisper.git\r\nopencv-python\r\nnumpy\r\npydantic\r\nflask\r\nfastapi\r\nuvicorn\r\npytest\r\n# optional for ROS2 integration (install on ROS machine):\r\n# rclpy  # install via ROS2 distribution\n"})}),"\n",(0,o.jsx)(n.h3,{id:"dockerfile",children:(0,o.jsx)(n.code,{children:"Dockerfile"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'FROM python:3.10-slim\r\nWORKDIR /app\r\nCOPY requirements.txt .\r\nRUN pip install --no-cache-dir -r requirements.txt\r\nCOPY . .\r\nEXPOSE 7860\r\nCMD ["python", "module4/demo/app.py"]\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.h2,{id:"docs-pages-docusaurus-ready-markdown",children:[(0,o.jsx)(n.code,{children:"docs/"})," pages (Docusaurus-ready markdown)"]}),"\n",(0,o.jsxs)(n.p,{children:["I included full markdown pages you can drop into ",(0,o.jsx)(n.code,{children:"docs/"})," in a Docusaurus site."]}),"\n",(0,o.jsx)(n.p,{children:"Files:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"module4-specs.md"})," \u2014 polished spec (your plan, restructured)"]}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"chapter-4-1-voice-to-action.md"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"chapter-4-2-cognitive-planning.md"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"chapter-4-3-capstone.md"})}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Each doc includes objectives, code pointers, and exercises."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"implementation-stubs-excerpts",children:"Implementation stubs (excerpts)"}),"\n",(0,o.jsxs)(n.p,{children:["Below are the actual code stubs included in ",(0,o.jsx)(n.code,{children:"module4/"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"module4voice_to_actionwhisper_integrationpy",children:(0,o.jsx)(n.code,{children:"module4/voice_to_action/whisper_integration.py"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# whisper_integration.py\r\n# Lightweight wrapper to run OpenAI Whisper locally and return text transcription.\r\n\r\nimport whisper\r\nfrom pathlib import Path\r\n\r\nMODEL_NAME = "small"\r\n\r\nclass WhisperWrapper:\r\n    def __init__(self, model_name=MODEL_NAME):\r\n        self.model = whisper.load_model(model_name)\r\n\r\n    def transcribe(self, audio_path: str) -> str:\r\n        result = self.model.transcribe(audio_path)\r\n        return result.get("text", "").strip()\r\n\r\nif __name__ == "__main__":\r\n    w = WhisperWrapper()\r\n    print(w.transcribe("../../sample_inputs/sample1.wav"))\n'})}),"\n",(0,o.jsx)(n.h3,{id:"module4voice_to_actionros2_bridgepy",children:(0,o.jsx)(n.code,{children:"module4/voice_to_action/ros2_bridge.py"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# ros2_bridge.py\r\n# NOTE: This is a stub for how to publish transcription -> ROS2 topic.\r\n# Real ROS2 usage requires running within a ROS2 Python environment (rclpy).\r\n\r\ntry:\r\n    import rclpy\r\n    from std_msgs.msg import String\r\nexcept Exception:\r\n    rclpy = None\r\n\r\nclass ROS2Bridge:\r\n    def __init__(self, node_name="vla_bridge"):\r\n        if rclpy is None:\r\n            raise RuntimeError("ROS2 (rclpy) not available in this environment")\r\n        rclpy.init()\r\n        self.node = rclpy.create_node(node_name)\r\n        self.pub = self.node.create_publisher(String, "/vla/transcription", 10)\r\n\r\n    def publish_transcription(self, text: str):\r\n        msg = String()\r\n        msg.data = text\r\n        self.pub.publish(msg)\r\n\r\n    def shutdown(self):\r\n        self.node.destroy_node()\r\n        rclpy.shutdown()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"module4plannerllm_plannerpy",children:(0,o.jsx)(n.code,{children:"module4/planner/llm_planner.py"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# llm_planner.py\r\n# Simple LLM planning wrapper that converts text commands into action JSON.\r\n\r\nimport os\r\nimport json\r\nfrom typing import List\r\n\r\nOPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")\r\n\r\n# This wrapper uses OpenAI\'s GPT-4 style API (pseudo) \u2014 in your code, wire it to real client.\r\n\r\ndef plan_actions_from_instruction(instruction: str) -> List[dict]:\r\n    """\r\n    Example output:\r\n    [\r\n      {"action": "navigate", "params": {"location": "kitchen"}},\r\n      {"action": "pick", "params": {"object": "cup"}}\r\n    ]\r\n    """\r\n    # For hackathon/poc, implement a deterministic rule-based fallback:\r\n    if "kitchen" in instruction.lower():\r\n        return [{"action": "navigate", "params": {"location": "kitchen"}}]\r\n    # Otherwise return a placeholder sequence\r\n    return [{"action": "noop", "params": {}}]\r\n\r\nif __name__ == "__main__":\r\n    print(json.dumps(plan_actions_from_instruction("Go to the kitchen and pick the cup"), indent=2))\n'})}),"\n",(0,o.jsx)(n.h3,{id:"module4visiondetectorpy",children:(0,o.jsx)(n.code,{children:"module4/vision/detector.py"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# detector.py\r\n# Lightweight wrapper using torchvision fasterrcnn or CLIP-based detection for quick demos.\r\n\r\nfrom PIL import Image\r\n\r\ndef detect_objects(image_path: str):\r\n    # For demo use: return mock detections\r\n    return [{"label": "cup", "bbox": [10,10,100,100], "score": 0.88}]\r\n\r\nif __name__ == "__main__":\r\n    print(detect_objects("../../sample_inputs/sample1.jpg"))\n'})}),"\n",(0,o.jsxs)(n.h3,{id:"module4demoapppy-gradio-demo-tying-everything-together",children:[(0,o.jsx)(n.code,{children:"module4/demo/app.py"})," (Gradio demo tying everything together)"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# app.py\r\nimport gradio as gr\r\nfrom pathlib import Path\r\nfrom module4.voice_to_action.whisper_integration import WhisperWrapper\r\nfrom module4.planner.llm_planner import plan_actions_from_instruction\r\nfrom module4.vision.detector import detect_objects\r\n\r\nwhisper = WhisperWrapper()\r\n\r\ndef run_demo(audio_file, image_file, typed_instruction):\r\n    transcripts = ""\r\n    if audio_file:\r\n        transcripts = whisper.transcribe(audio_file.name)\r\n    instruction = typed_instruction or transcripts\r\n    plan = plan_actions_from_instruction(instruction)\r\n    detections = detect_objects(image_file.name) if image_file else []\r\n    return transcripts, plan, detections\r\n\r\nwith gr.Blocks() as demo:\r\n    gr.Markdown("# VLA Demo \u2014 Voice \u2192 Language \u2192 Action")\r\n    with gr.Row():\r\n        audio = gr.Audio(label="Upload voice command (wav)", source="upload")\r\n        image = gr.Image(label="Optional scene image", type="filepath")\r\n    typed = gr.Textbox(label="Or type instruction")\r\n    out_text = gr.Textbox(label="Transcript")\r\n    out_plan = gr.JSON(label="Planned actions")\r\n    out_dets = gr.JSON(label="Detections")\r\n    btn = gr.Button("Run")\r\n    btn.click(lambda a,i,t: run_demo(a,i,t), inputs=[audio,image,typed], outputs=[out_text,out_plan,out_dets])\r\n\r\nif __name__ == "__main__":\r\n    demo.launch(server_name="0.0.0.0")\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.h2,{id:"docsmodule4-specsmd-polished-docusaurus-page",children:[(0,o.jsx)(n.code,{children:"docs/module4-specs.md"})," (polished Docusaurus page)"]}),"\n",(0,o.jsxs)(n.p,{children:["This markdown page contains the Module 4 spec (your provided plan) but restructured with objectives, deliverables, prerequisites, and timeline. Drop into a Docusaurus ",(0,o.jsx)(n.code,{children:"docs/"})," folder."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"checklistmd-submission",children:"CHECKLIST.md (submission)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"# Module 4 \u2014 Submission Checklist\r\n\r\n- [ ] Demo link or container image\r\n- [ ] README with run instructions (10-minute run requirement)\r\n- [ ] 1\u20133 minute demo video\r\n- [ ] Short one-pager: problem, approach, models, datasets, eval\r\n- [ ] Risk & ethics short note\n"})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"how-i-generated-these-files",children:"How I generated these files"}),"\n",(0,o.jsxs)(n.p,{children:["I used your ",(0,o.jsx)(n.code,{children:"plan.md"})," structure as the canonical spec and produced:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Docusaurus-ready docs pages"}),"\n",(0,o.jsx)(n.li,{children:"A runnable (minimal) Gradio demo tying Whisper -> LLM planner -> Vision detector"}),"\n",(0,o.jsx)(n.li,{children:"ROS2 bridge stubs that show how to integrate (require ROS2 environment at runtime)"}),"\n",(0,o.jsx)(n.li,{children:"Dockerfile + requirements for quick packaging"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"All code stubs are intentionally minimal so they run on CPU for demo purposes and are easy to replace with real models."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"next-steps-pick-one",children:"Next steps (pick one)"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"I can generate each file as separate editable canvas docs (so you can copy them directly into your repo). (Choose this if you want file-by-file outputs.)"}),"\n",(0,o.jsx)(n.li,{children:"I can generate a downloadable ZIP of the scaffold (if you want to run locally). (I will produce code in the canvas and provide instructions.)"}),"\n",(0,o.jsx)(n.li,{children:"I can expand any module (Whisper integration, LLM planning, ROS2 bridge, or Gradio demo) into full production-quality code."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Tell me which of the three you want and I\u2019ll produce it now."})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>d});var i=r(6540);const o={},t=i.createContext(o);function s(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);